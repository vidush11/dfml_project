{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import copy\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import random\n",
    "import json\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import DirichletPartitioner,PathologicalPartitioner\n",
    "from flwr_datasets.preprocessor import Divider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 11\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "device = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we just load and visualize the data from FEMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"flwrlabs/femnist\")\n",
    "\n",
    "# partitioner = DirichletPartitioner(num_partitions=10, partition_by=\"character\",\n",
    "#                                    alpha=0.5, min_partition_size=10,\n",
    "#                                    self_balancing=False,seed=SEED)\n",
    "\n",
    "# fds = FederatedDataset(dataset=\"flwrlabs/femnist\", partitioners={\"train\": partitioner})\n",
    "\n",
    "# partitioner = PathologicalPartitioner(\n",
    "#     num_partitions=62,\n",
    "#     partition_by=\"character\",\n",
    "#     num_classes_per_partition=1,\n",
    "#     class_assignment_mode=\"first-deterministic\"\n",
    "# )\n",
    "\n",
    "# fds = FederatedDataset(dataset=\"flwrlabs/femnist\",preprocessor=Divider(divide_config={\"train\": 0.8,\"test\": 0.2,}, divide_split=\"train\",), partitioners={\"train\": partitioner})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(set(fds.load_partition(partition_id=0)[i]['character'] for i in range(len(fds.load_partition(partition_id=0)))))\n",
    "# lens=[[0]*62 for _ in range(100)]\n",
    "\n",
    "# for _ in range(100):\n",
    "#     partition = fds.load_partition(partition_id=_)\n",
    "#     for i in range(len(partition)):\n",
    "#         lens[_][partition[i][\"character\"]]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fds.load_partition(split=\"train\",partition_id=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(60*\"=\")\n",
    "train_ds = dataset[\"train\"]\n",
    "print(f\"We have {len(train_ds)} data samples.\")\n",
    "example = train_ds[0]\n",
    "fig,axs=plt.subplots(10,10, tight_layout = True)\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        axs[i,j].imshow(train_ds[i*10+j][\"image\"], cmap='gray')\n",
    "        axs[i,j].set_xticks([])\n",
    "        axs[i,j].set_yticks([])\n",
    "        axs[i,j].text(0.2, 0.2, train_ds[i*10+j][\"character\"], horizontalalignment='center', verticalalignment='center', transform=axs[i,j].transAxes)\n",
    "print(\"The keys of dictionary are-\\n\",example.keys())           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = train_ds.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_data = split[\"train\"]\n",
    "test_data = split[\"test\"]  \n",
    "\n",
    "print(f\"Samples in train - {len(train_data)}, Samples in test - {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Task1</h3>\n",
    "Now the task is to give the training samples to each client<br>\n",
    "We will do this by partitioning the data using a dirichlet prior<br>\n",
    "Dirichlet acts like a prior over <b>class proportions per client.</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = len(train_data.features[\"character\"].names)\n",
    "num_classes=62\n",
    "num_clients = 62\n",
    "print(num_classes, num_clients)\n",
    "\n",
    "# labels = np.array(train_data[\"character\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01 #larger value promotes uniform distribution smaller alpha means skewed distribution\n",
    "\n",
    "# Grouping indices with the same class together\n",
    "indices_by_class=[np.where(labels==i)[0] for i in range(num_classes)]\n",
    "\n",
    "# client_data[c][i] = indices for class c assigned to client i\n",
    "client_data = dict()\n",
    "print(\"Number of data samples per class\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"{i}->{len(indices_by_class[i])}\")\n",
    "# print([len(x) for x in indices_by_class])\n",
    "\n",
    "print(\"Distribution of classes per client\")\n",
    "for c in range(num_classes):\n",
    "    client_data[c]=[]\n",
    "    samples_in_class = indices_by_class[c]\n",
    "    num_samples_in_class = len(samples_in_class)\n",
    "    if num_samples_in_class == 0:\n",
    "        continue\n",
    "\n",
    "    # Dirichlet proportions for this class across clients\n",
    "    p = np.random.dirichlet(alpha * np.ones(num_clients))\n",
    "\n",
    "    # Integer client counts\n",
    "    counts = np.random.multinomial(num_samples_in_class, p)\n",
    "    prev=0\n",
    "    for i in range(num_clients):\n",
    "        client_data[c].append(indices_by_class[c][prev:prev+counts[i]])\n",
    "        prev+=counts[i]\n",
    "    # # OPTIONAL: Print summary\n",
    "    print(f\"Class {c}: {[chunk.shape[0] for chunk in client_data[c]]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per class label distribution across clients plots\")\n",
    "\n",
    "for c in range(num_classes):\n",
    "    assert (c in client_data)\n",
    "    chunks = client_data[c]  # list of indice arrays for class c for each client\n",
    "    \n",
    "    counts = np.array([len(chunk) for chunk in chunks], dtype=int)\n",
    "    total = counts.sum()\n",
    "    assert(total==indices_by_class[c].shape[0])\n",
    "\n",
    "    print(\n",
    "        f\"Class{c:2d} |Total={total:5d} |\"\n",
    "        f\"min={counts.min():4d} |max={counts.max():6d} |\"\n",
    "        f\"mean={counts.mean():7.2f} |std={counts.std():8.2f} |\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_list = []\n",
    "classes_to_plot=list(range(5))\n",
    "n_plot=5\n",
    "for c in range(5):\n",
    "    per_client_data = client_data[c]\n",
    "    counts_list.append(np.array([len(client) for client in per_client_data]))\n",
    "    \n",
    "fig, axes = plt.subplots(1, 5, figsize=(6 * n_plot, 4), sharey=True)\n",
    "if n_plot == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, c, counts in zip(axes, classes_to_plot, counts_list):\n",
    "    frac = counts / sum(counts) \n",
    "    ax.bar(np.arange(len(frac)), frac)\n",
    "    ax.set_title(f\"Class {c}\")\n",
    "    ax.set_xlabel(\"Client ID\")\n",
    "    ax.set_xticks(np.arange(len(frac)))\n",
    "    ax.set_ylim(0, 1.0)                   # same y-axis for all\n",
    "\n",
    "axes[0].set_ylabel(\"Fraction of this class\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FemnistCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        # Input: (N, 1, 28, 28)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2, stride=2)  # halves H and W\n",
    "        self.dropout2d = nn.Dropout2d(p=0.25)\n",
    "\n",
    "        # After two pools: 28 -> 14 -> 7, channels = 64\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.conv1.bias)\n",
    "        nn.init.zeros_(self.conv2.bias)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (N, 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)       # (N, 32, 14, 14)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)       # (N, 64, 7, 7)\n",
    "        x = self.dropout2d(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)  # (N, 64*7*7)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)          # (N, num_classes)\n",
    "        return x\n",
    "    \n",
    "model = FemnistCNN(num_classes).to(device)\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "test_input = torch.randn(4, 1, 28, 28).to(device)\n",
    "test_output = model(test_input)\n",
    "\n",
    "print(f\"\\nTest input shape: {test_input.shape}\")\n",
    "print(f\"Test output shape: {test_output.shape}\")\n",
    "\n",
    "assert(test_output.shape==(4,num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build per-client index lists from the per-class Dirichlet splits (client_data)\n",
    "\n",
    "client_indices = [np.hstack([client_data[i][j] for i in range(num_classes)]) for j in range(num_clients)]\n",
    "\n",
    "#Shuffle indices within each client for randomness\n",
    "for cid in range(num_clients):\n",
    "    np.random.shuffle(client_indices[cid])\n",
    "\n",
    "# Sanity check\n",
    "assigned = [len(idxs) for idxs in client_indices]\n",
    "print(\"Distribution of data samples per client is\", assigned)\n",
    "print(f\"Total samples assigned to clients: {sum(assigned)}\")\n",
    "print(f\"Train set size: {len(train_data)}\")\n",
    "\n",
    "assert(sum(assigned)==len(train_data))\n",
    "print(np.asarray(train_data[0][\"image\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),   \n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, dataset, indices, transform=transform):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[self.indices[idx]]\n",
    "        img = item[\"image\"]       # PIL image\n",
    "        label = item[\"character\"]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)  # -> (1, 28, 28) tensor\n",
    "        return (img, label)\n",
    "\n",
    "class client():\n",
    "    def __init__(self,client_id:int,dataset:Custom_Dataset, model:FemnistCNN, optimizer:str, lr:float):\n",
    "        self.client_id=client_id\n",
    "        self.dataset=dataset\n",
    "        self.dataloader=DataLoader(self.dataset, batch_size=640, shuffle=True)\n",
    "        self.model=model\n",
    "        self.lr=lr\n",
    "        self.name=optimizer.lower()\n",
    "        self.optimizer=self._get_optimizer(optimizer.lower())\n",
    "        self.losses=[]\n",
    "        self.fedprox=False\n",
    "        self.u=1\n",
    "        \n",
    "    def _get_optimizer(self, name):\n",
    "        params=self.model.parameters()\n",
    "        if name==\"sgd\":\n",
    "            return optim.SGD(params, lr=self.lr)\n",
    "        elif name == \"sgd_momentum\":\n",
    "            return optim.SGD(params, lr=self.lr, momentum=0.9, nesterov=False)\n",
    "        elif name == \"adam\":\n",
    "            return optim.Adam(params)\n",
    "        elif name == \"adamw\":\n",
    "            return optim.AdamW(params)\n",
    "        elif name == \"rmsprop\":\n",
    "            return optim.RMSprop(params)\n",
    "        elif name == \"radam\":\n",
    "            return optim.RAdam(params)\n",
    "        elif name==\"nadam\":\n",
    "            return optim.NAdam(params)\n",
    "        elif name==\"fedprox\":\n",
    "            self.fedprox=True\n",
    "            return optim.SGD(params, lr=self.lr)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {name}\")\n",
    "    def train(self,epochs, status=False):\n",
    "        # for _ in range(epochs):\n",
    "        # print(len(self.dataloader))\n",
    "        i=0\n",
    "        # items=tqdm(iter(self.dataloader), \", leave=False )\n",
    "        w_ = {name: param.clone() for name, param in self.model.state_dict().items()}\n",
    "        self.model.train()\n",
    "        for x,y in iter(self.dataloader):\n",
    "            x=x.to(device)\n",
    "            y=y.to(device)\n",
    "            i+=1\n",
    "            if (status and (i%5==0)):\n",
    "                print(f\"Client {self.client_id} | Epoch {i}/{epochs}\")\n",
    "            outputs=self.model(x)\n",
    "            loss=torch.nn.CrossEntropyLoss()(outputs,y)\n",
    "            if (self.fedprox):\n",
    "                for name,param in w_.items():\n",
    "                    loss+=self.u*((param-self.model.state_dict()[name])**2).sum()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.losses.append(loss.item())\n",
    "            if i==epochs:\n",
    "                break\n",
    "\n",
    "    def send_to_server(self):\n",
    "        state = {name: param.clone() for name, param in self.model.state_dict().items()}\n",
    "        n = len(self.dataset)\n",
    "        return (state,n)\n",
    "\n",
    "    def receive_from_server(self, param_dict):\n",
    "        state={name: param.clone() for name, param in param_dict.items()}\n",
    "        self.model.load_state_dict(state)\n",
    "\n",
    "    def set_learning_rate(self, lr):\n",
    "        self.optimizer.lr=lr\n",
    "class server():\n",
    "    def __init__(self, clients:[client], dataset:Custom_Dataset, model, optimizer:str, lr:float):\n",
    "        self.clients=clients\n",
    "        self.optimizer=optimizer.lower()\n",
    "        self.model=model\n",
    "        self.dataset=dataset\n",
    "        self.dataloader=DataLoader(self.dataset, batch_size=1280, shuffle=True)\n",
    "        self.X=None\n",
    "        self.Y=None\n",
    "        self.lr=lr\n",
    "        self.losses=[]\n",
    "        self.predictions_correct=[]\n",
    "        self.local_rounds=1000\n",
    "        self.path=f\"c1_{self.clients[0].name}\"\n",
    "        self.optimizer_dict=None\n",
    "    def global_round(self):\n",
    "        for client in self.clients:\n",
    "            client.receive_from_server(self.model.state_dict())   \n",
    "        #put here random indices for client sampling\n",
    "        for client in self.clients:\n",
    "            client.train(self.local_rounds,True) #number of epochs\n",
    "        s=0\n",
    "        client_weights=[]\n",
    "        for client in self.clients:\n",
    "            client_weights.append(client.send_to_server())\n",
    "            s+=client_weights[-1][1]\n",
    "\n",
    "        if self.optimizer==\"sgd\":\n",
    "            new_weight_dict={k:torch.zeros_like(v) for k,v in client_weights[0][0].items()}\n",
    "            for i in range(len(client_weights)):\n",
    "                client_dict=client_weights[i][0]\n",
    "                client_samples=client_weights[i][1]\n",
    "                for k,v in client_dict.items():\n",
    "                    new_weight_dict[k]+=v*client_samples/s\n",
    "            self.model.load_state_dict(new_weight_dict)\n",
    "            \n",
    "        elif self.optimizer==\"adam\":\n",
    "            if (self.optimizer_dict is None):\n",
    "                self.optimizer_dict={}\n",
    "            self.optimizer_dict.setdefault('learning_rate', 1e-1)\n",
    "            self.optimizer_dict.setdefault('beta1', 0.9)\n",
    "            self.optimizer_dict.setdefault('beta2', 0.999)\n",
    "            self.optimizer_dict.setdefault('epsilon', 1e-8)\n",
    "            self.optimizer_dict.setdefault('m', {k:torch.zeros_like(v) for k,v in client_weights[0][0].items()})\n",
    "            self.optimizer_dict.setdefault('v', {k:torch.zeros_like(v) for k,v in client_weights[0][0].items()})\n",
    "            self.optimizer_dict.setdefault('t', 0)\n",
    "            new_weight_dict={k:torch.zeros_like(v) for k,v in client_weights[0][0].items()}\n",
    "            for i in range(len(client_weights)):\n",
    "                client_dict=client_weights[i][0]\n",
    "                client_samples=client_weights[i][1]\n",
    "                for k,v in client_dict.items():\n",
    "                    new_weight_dict[k]+=v*client_samples/len(self.dataset)\n",
    "            self.optimizer_dict['t']+=1\n",
    "            for k,v in client_weights[0][0].items():\n",
    "                w=self.model.state_dict()[k]\n",
    "                new_weight_dict[k]-=w\n",
    "                m=self.optimizer_dict['m']\n",
    "                v=self.optimizer_dict['v']\n",
    "                m[k]=self.optimizer_dict['beta1']*m[k]+(1-self.optimizer_dict['beta1'])*new_weight_dict[k]\n",
    "                v[k]=self.optimizer_dict['beta2']*v[k]+(1-self.optimizer_dict['beta2'])*new_weight_dict[k]*new_weight_dict[k]\n",
    "                m_ub=m[k]/(1-self.optimizer_dict['beta1']**self.optimizer_dict['t'])\n",
    "                v_ub=v[k]/(1-self.optimizer_dict['beta2']**self.optimizer_dict['t'])\n",
    "                next_w=w+self.optimizer_dict['learning_rate']*m_ub/(self.optimizer_dict['epsilon']+v_ub.sqrt())\n",
    "                new_weight_dict[k]=next_w\n",
    "            self.model.load_state_dict(new_weight_dict)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown global optimizer: {name}\")\n",
    "            \n",
    "    def train(self, rounds):\n",
    "        for i in range(rounds):\n",
    "            print(f\"Running round {i+1}/{rounds} on server.\")\n",
    "            self.global_round()\n",
    "            self.evaluate(50)\n",
    "            print(f\"Loss:{self.losses[-1]}, Accuracy: {self.predictions_correct[-1]*100}%\")\n",
    "            self.save_state()\n",
    "        self.evaluate(len(self.dataset)//1280+1)\n",
    "        print(f\"Final Loss:{self.losses[-1]}, Accuracy: {self.predictions_correct[-1]*100}%\")\n",
    "        \n",
    "    def evaluate(self,batches):\n",
    "        self.model.eval()\n",
    "        loss=0\n",
    "        correct=0\n",
    "        total=0\n",
    "        i=0\n",
    "        for x,y in iter(self.dataloader):\n",
    "            x=x.to(device)\n",
    "            y=y.to(device)\n",
    "            i+=1\n",
    "            outputs=self.model(x)\n",
    "            loss_=torch.nn.CrossEntropyLoss()(outputs,y)\n",
    "            loss+=loss_.item()\n",
    "            correct+=(outputs.argmax(axis=1)==y).sum()\n",
    "            total+=y.shape[0]\n",
    "            if (i==batches):\n",
    "                break\n",
    "        self.losses.append(loss)\n",
    "        self.predictions_correct.append((correct/total).item())\n",
    "\n",
    "    def save_state(self):\n",
    "        torch.save(self.model.state_dict(), f\"./{self.path}.pt\")\n",
    "        with open(f\"./vs-code/losses_{self.path}.json\", 'w') as file:\n",
    "            file.write(json.dumps(self.losses))\n",
    "        with open(f\"./vs-code/training_accuracy_{self.path}.json\", 'w') as file:\n",
    "            file.write(json.dumps([a for a in self.predictions_correct]))\n",
    "    def load_state(self):\n",
    "        self.model.load_state_dict(torch.load(f\"./{self.path}.pt\", weights_only=True))\n",
    "        with open(f\"./vs-code/losses_{self.path}.json\", 'r') as file:\n",
    "            self.losses=json.loads(file.read())\n",
    "        with open(f\"./vs-code/training_accuracy_{self.path}.json\", 'r') as file:\n",
    "            self.predictions_correct=json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients=[client(i+1,Custom_Dataset(fds.load_partition(split=\"train\",partition_id=i),list(range(len(fds.load_partition(split=\"train\",partition_id=i))))),FemnistCNN(num_classes).to(device),\"sgd_momentum\",0.001) for i in range(num_clients)]\n",
    "myServer=server(clients,Custom_Dataset(fds.load_split(\"test\"),list(range(len(fds.load_split(\"test\"))))),FemnistCNN(num_classes).to(device),\"sgd\",0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try:\n",
    "#     myServer.load_state()\n",
    "#     myServer.train(1000)\n",
    "# except KeyboardInterrupt:\n",
    "#     myServer.save_state()\n",
    "# for client in clients:\n",
    "#     client.lr=0.1\n",
    "#     client._get_optimizer(\"adam\")\n",
    "\n",
    "myServer.train(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myServer.save_state()\n",
    "# for client in clients:\n",
    "#     client.lr=0.1\n",
    "#     client._get_optimizer(\"fedprox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% [code]\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# results_csv = \"fedavg_results_with_trainloss.csv\"\n",
    "# df = pd.read_csv(results_csv)\n",
    "\n",
    "# rounds = df[\"round\"]\n",
    "\n",
    "# # --- Training Loss vs Rounds ---\n",
    "# plt.figure(figsize=(6,4))\n",
    "# plt.plot(rounds, df[\"train_loss\"], linewidth=2)\n",
    "# plt.title(\"Training Loss vs Rounds\")\n",
    "# plt.xlabel(\"Round\")\n",
    "# plt.ylabel(\"Training Loss\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # --- Test Loss vs Rounds ---\n",
    "# plt.figure(figsize=(6,4))\n",
    "# plt.plot(rounds, df[\"global_loss\"], linewidth=2)\n",
    "# plt.title(\"Test Loss vs Rounds\")\n",
    "# plt.xlabel(\"Round\")\n",
    "# plt.ylabel(\"Test Loss\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # --- Test Accuracy vs Rounds ---\n",
    "# plt.figure(figsize=(6,4))\n",
    "# plt.plot(rounds, df[\"global_acc\"], linewidth=2)\n",
    "# plt.title(\"Test Accuracy vs Rounds\")\n",
    "# plt.xlabel(\"Round\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
