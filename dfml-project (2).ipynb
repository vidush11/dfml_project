{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-16T12:26:38.636002Z",
     "iopub.status.busy": "2025-11-16T12:26:38.635671Z",
     "iopub.status.idle": "2025-11-16T12:26:44.530870Z",
     "shell.execute_reply": "2025-11-16T12:26:44.530249Z",
     "shell.execute_reply.started": "2025-11-16T12:26:38.635978Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import copy\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T12:26:44.532368Z",
     "iopub.status.busy": "2025-11-16T12:26:44.531955Z",
     "iopub.status.idle": "2025-11-16T12:26:44.618825Z",
     "shell.execute_reply": "2025-11-16T12:26:44.618179Z",
     "shell.execute_reply.started": "2025-11-16T12:26:44.532347Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 69 ; torch.manual_seed(SEED) ; np.random.seed(SEED) ; random.seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we just load and visualize the data from FEMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T12:26:44.619782Z",
     "iopub.status.busy": "2025-11-16T12:26:44.619546Z",
     "iopub.status.idle": "2025-11-16T12:26:52.265516Z",
     "shell.execute_reply": "2025-11-16T12:26:52.264752Z",
     "shell.execute_reply.started": "2025-11-16T12:26:44.619764Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'flwrlabs/femnist' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48172cf4de67423fa81c13ad07dfeb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3146f6c6aed246ebbe2390dc0d49835f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/201M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c975f201bc64683ad8eac19a4e9d5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/814277 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"flwrlabs/femnist\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T12:26:52.267867Z",
     "iopub.status.busy": "2025-11-16T12:26:52.267417Z",
     "iopub.status.idle": "2025-11-16T12:26:52.311761Z",
     "shell.execute_reply": "2025-11-16T12:26:52.311036Z",
     "shell.execute_reply.started": "2025-11-16T12:26:52.267846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'writer_id', 'hsf_id', 'character'],\n",
      "        num_rows: 814277\n",
      "    })\n",
      "})\n",
      "==============================\n",
      "dict_keys(['image', 'writer_id', 'hsf_id', 'character'])\n",
      "0\n",
      "f0000_14\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(30*\"=\")\n",
    "train_ds = dataset[\"train\"]\n",
    "example = train_ds[0]\n",
    "print(example.keys())          \n",
    "print(example[\"character\"])    \n",
    "print(example[\"writer_id\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T12:26:52.312866Z",
     "iopub.status.busy": "2025-11-16T12:26:52.312515Z",
     "iopub.status.idle": "2025-11-16T12:26:52.561614Z",
     "shell.execute_reply": "2025-11-16T12:26:52.560950Z",
     "shell.execute_reply.started": "2025-11-16T12:26:52.312846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in train - 651421, Samples in test - 162856\n"
     ]
    }
   ],
   "source": [
    "split = train_ds.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_data = split[\"train\"] ; test_data = split[\"test\"]  \n",
    "\n",
    "print(f\"Samples in train - {len(train_data)}, Samples in test - {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the task is to give the training samples to each client \n",
    "\n",
    "We will do this by partitioning the data using a dirichlet prior \n",
    "\n",
    "Dirichlet acts like a prior over **class proportions per client.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T12:26:52.562722Z",
     "iopub.status.busy": "2025-11-16T12:26:52.562468Z",
     "iopub.status.idle": "2025-11-16T12:26:52.567852Z",
     "shell.execute_reply": "2025-11-16T12:26:52.566975Z",
     "shell.execute_reply.started": "2025-11-16T12:26:52.562698Z"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = len(train_data.features[\"character\"].names) ; num_clients = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T12:26:52.569097Z",
     "iopub.status.busy": "2025-11-16T12:26:52.568724Z",
     "iopub.status.idle": "2025-11-16T12:27:21.319510Z",
     "shell.execute_reply": "2025-11-16T12:27:21.318797Z",
     "shell.execute_reply.started": "2025-11-16T12:26:52.569070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: [1512, 7010, 1753, 6728, 3507, 1275, 459, 246, 4783, 5075]\n",
      "Class 1: [1427, 2444, 15015, 2615, 3898, 1724, 461, 4285, 1782, 2012]\n",
      "Class 2: [2794, 4934, 1463, 1971, 440, 1963, 334, 7355, 10080, 923]\n",
      "Class 3: [1338, 6, 1157, 648, 2998, 7542, 6601, 5183, 1619, 5892]\n",
      "Class 4: [3020, 10797, 3455, 1815, 774, 6680, 921, 300, 3484, 144]\n",
      "Class 5: [5216, 1645, 4431, 2391, 1182, 3245, 5015, 338, 3841, 1984]\n",
      "Class 6: [3019, 290, 170, 8359, 2216, 11713, 3484, 220, 2350, 178]\n",
      "Class 7: [9088, 1424, 152, 7363, 1439, 6595, 617, 5301, 1404, 119]\n",
      "Class 8: [473, 11263, 2388, 105, 1043, 2896, 2128, 2724, 8181, 557]\n",
      "Class 9: [4389, 1401, 9006, 5543, 2998, 1358, 773, 138, 147, 5849]\n",
      "Class 10: [1360, 550, 361, 1072, 21, 576, 333, 34, 1533, 84]\n",
      "Class 11: [9, 441, 299, 180, 642, 699, 97, 411, 812, 15]\n",
      "Class 12: [671, 2958, 2229, 582, 515, 243, 1644, 357, 124, 101]\n",
      "Class 13: [917, 113, 181, 249, 314, 382, 517, 1077, 305, 225]\n",
      "Class 14: [32, 496, 42, 592, 72, 376, 5, 944, 507, 1566]\n",
      "Class 15: [2285, 1350, 626, 638, 529, 438, 713, 1066, 351, 512]\n",
      "Class 16: [12, 514, 425, 273, 23, 138, 71, 381, 209, 312]\n",
      "Class 17: [523, 107, 208, 464, 50, 473, 247, 433, 247, 212]\n",
      "Class 18: [2075, 106, 2256, 1541, 3191, 160, 758, 500, 155, 419]\n",
      "Class 19: [126, 492, 950, 344, 162, 58, 261, 451, 481, 173]\n",
      "Class 20: [278, 325, 138, 12, 288, 17, 62, 310, 675, 184]\n",
      "Class 21: [46, 829, 406, 313, 1422, 480, 99, 834, 182, 78]\n",
      "Class 22: [355, 656, 2953, 110, 1460, 12, 581, 411, 1256, 663]\n",
      "Class 23: [2372, 1024, 976, 256, 465, 196, 675, 331, 1065, 296]\n",
      "Class 24: [1980, 4491, 336, 3662, 176, 1678, 42, 6499, 749, 3597]\n",
      "Class 25: [717, 555, 1064, 2465, 650, 262, 1195, 36, 669, 185]\n",
      "Class 26: [14, 61, 177, 142, 304, 11, 447, 248, 640, 344]\n",
      "Class 27: [389, 626, 126, 72, 651, 992, 24, 1067, 711, 40]\n",
      "Class 28: [155, 35, 966, 2858, 2669, 1324, 277, 170, 2737, 8080]\n",
      "Class 29: [1316, 304, 1512, 1200, 1004, 145, 626, 254, 1927, 812]\n",
      "Class 30: [687, 2937, 1145, 2868, 49, 1681, 112, 268, 1424, 459]\n",
      "Class 31: [513, 148, 364, 869, 284, 406, 26, 679, 191, 933]\n",
      "Class 32: [181, 576, 428, 1323, 460, 177, 178, 360, 625, 63]\n",
      "Class 33: [502, 476, 559, 62, 495, 54, 36, 292, 3, 51]\n",
      "Class 34: [573, 186, 45, 739, 6, 211, 749, 346, 24, 1537]\n",
      "Class 35: [72, 120, 100, 74, 40, 783, 34, 18, 590, 646]\n",
      "Class 36: [569, 154, 973, 2700, 2670, 30, 948, 306, 542, 490]\n",
      "Class 37: [509, 418, 26, 852, 415, 1061, 359, 151, 728, 266]\n",
      "Class 38: [189, 353, 716, 63, 226, 226, 52, 19, 739, 38]\n",
      "Class 39: [378, 499, 608, 46, 4320, 96, 3065, 258, 263, 25]\n",
      "Class 40: [5879, 213, 928, 2380, 4899, 3229, 434, 3390, 840, 775]\n",
      "Class 41: [78, 39, 727, 368, 389, 140, 322, 115, 10, 177]\n",
      "Class 42: [35, 558, 83, 177, 952, 584, 2, 286, 100, 668]\n",
      "Class 43: [1742, 1986, 8, 1424, 407, 432, 96, 5, 772, 1241]\n",
      "Class 44: [173, 73, 89, 789, 193, 51, 47, 43, 936, 142]\n",
      "Class 45: [404, 151, 103, 290, 324, 361, 72, 2, 38, 50]\n",
      "Class 46: [419, 239, 99, 7, 263, 351, 271, 38, 270, 393]\n",
      "Class 47: [30, 456, 1147, 382, 2065, 4606, 1575, 636, 1562, 1796]\n",
      "Class 48: [681, 598, 24, 248, 335, 69, 269, 130, 132, 5]\n",
      "Class 49: [1652, 968, 2922, 696, 1247, 208, 748, 137, 766, 1386]\n",
      "Class 50: [145, 428, 42, 55, 245, 119, 521, 345, 659, 36]\n",
      "Class 51: [672, 178, 501, 542, 54, 58, 11, 65, 12, 152]\n",
      "Class 52: [251, 243, 654, 226, 165, 208, 146, 67, 656, 193]\n",
      "Class 53: [15, 321, 1528, 37, 977, 1240, 3362, 4903, 469, 253]\n",
      "Class 54: [29, 448, 314, 644, 109, 96, 152, 151, 258, 293]\n",
      "Class 55: [750, 107, 551, 2573, 1833, 2700, 1261, 3993, 2015, 1164]\n",
      "Class 56: [86, 904, 514, 14, 65, 9, 463, 61, 465, 113]\n",
      "Class 57: [287, 475, 157, 79, 95, 1161, 74, 81, 69, 223]\n",
      "Class 58: [136, 350, 213, 530, 213, 307, 78, 355, 346, 39]\n",
      "Class 59: [0, 252, 353, 79, 185, 246, 1032, 281, 58, 125]\n",
      "Class 60: [151, 195, 260, 65, 8, 484, 258, 172, 131, 476]\n",
      "Class 61: [177, 592, 49, 403, 613, 49, 68, 252, 7, 312]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels = np.array(train_data[\"character\"])\n",
    "num_classes = int(labels.max()) + 1  # or given\n",
    "num_clients = num_clients\n",
    "alpha = 1.0\n",
    "\n",
    "# One-time grouping of indices per class\n",
    "indices_by_class = [[] for _ in range(num_classes)]\n",
    "for idx, y in enumerate(labels):\n",
    "    indices_by_class[y].append(idx)\n",
    "indices_by_class = [np.array(idx_list, dtype=int) for idx_list in indices_by_class]\n",
    "\n",
    "# For storing per-class splits per client if needed\n",
    "# client_data[c][i] = indices for class c assigned to client i\n",
    "client_data = {}  # only store classes that actually appear\n",
    "\n",
    "for c in range(num_classes):\n",
    "    samples_in_class = indices_by_class[c]\n",
    "    num_samples_in_class = len(samples_in_class)\n",
    "    if num_samples_in_class == 0:\n",
    "        continue\n",
    "\n",
    "    # Dirichlet proportions for this class across clients\n",
    "    p = np.random.dirichlet(alpha * np.ones(num_clients))\n",
    "\n",
    "    # Integer client counts that exactly sum to num_samples_in_class\n",
    "    counts = np.random.multinomial(num_samples_in_class, p)\n",
    "\n",
    "    # Compute split points and partition the indices\n",
    "    split_points = np.cumsum(counts)[:-1]\n",
    "    client_vs_id_split = np.split(samples_in_class, split_points)\n",
    "\n",
    "    client_data[c] = client_vs_id_split\n",
    "\n",
    "    # OPTIONAL: Print summary\n",
    "    print(f\"Class {c}: {[len(chunk) for chunk in client_vs_id_split]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T12:27:21.320652Z",
     "iopub.status.busy": "2025-11-16T12:27:21.320368Z",
     "iopub.status.idle": "2025-11-16T12:27:21.331031Z",
     "shell.execute_reply": "2025-11-16T12:27:21.330246Z",
     "shell.execute_reply.started": "2025-11-16T12:27:21.320625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Per-class label distribution across clients ===\n",
      "Class  0 | total=32348 | min= 246 max=7010 mean=3234.80 std=2407.37 | per-client=[1512, 7010, 1753, 6728, 3507, 1275, 459, 246, 4783, 5075]\n",
      "Class  1 | total=35663 | min= 461 max=15015 mean=3566.30 std=3963.29 | per-client=[1427, 2444, 15015, 2615, 3898, 1724, 461, 4285, 1782, 2012]\n",
      "Class  2 | total=32257 | min= 334 max=10080 mean=3225.70 std=3079.57 | per-client=[2794, 4934, 1463, 1971, 440, 1963, 334, 7355, 10080, 923]\n",
      "Class  3 | total=32984 | min=   6 max=7542 mean=3298.40 std=2615.30 | per-client=[1338, 6, 1157, 648, 2998, 7542, 6601, 5183, 1619, 5892]\n",
      "Class  4 | total=31390 | min= 144 max=10797 mean=3139.00 std=3173.52 | per-client=[3020, 10797, 3455, 1815, 774, 6680, 921, 300, 3484, 144]\n",
      "Class  5 | total=29288 | min= 338 max=5216 mean=2928.80 std=1592.66 | per-client=[5216, 1645, 4431, 2391, 1182, 3245, 5015, 338, 3841, 1984]\n",
      "Class  6 | total=31999 | min= 170 max=11713 mean=3199.90 std=3695.30 | per-client=[3019, 290, 170, 8359, 2216, 11713, 3484, 220, 2350, 178]\n",
      "Class  7 | total=33502 | min= 119 max=9088 mean=3350.20 std=3203.89 | per-client=[9088, 1424, 152, 7363, 1439, 6595, 617, 5301, 1404, 119]\n",
      "Class  8 | total=31758 | min= 105 max=11263 mean=3175.80 std=3472.72 | per-client=[473, 11263, 2388, 105, 1043, 2896, 2128, 2724, 8181, 557]\n",
      "Class  9 | total=31602 | min= 138 max=9006 mean=3160.20 std=2808.41 | per-client=[4389, 1401, 9006, 5543, 2998, 1358, 773, 138, 147, 5849]\n",
      "Class 10 | total= 5924 | min=  21 max=1533 mean=592.40 std=521.88 | per-client=[1360, 550, 361, 1072, 21, 576, 333, 34, 1533, 84]\n",
      "Class 11 | total= 3605 | min=   9 max= 812 mean=360.50 std=274.85 | per-client=[9, 441, 299, 180, 642, 699, 97, 411, 812, 15]\n",
      "Class 12 | total= 9424 | min= 101 max=2958 mean=942.40 std=938.40 | per-client=[671, 2958, 2229, 582, 515, 243, 1644, 357, 124, 101]\n",
      "Class 13 | total= 4280 | min= 113 max=1077 mean=428.00 std=305.28 | per-client=[917, 113, 181, 249, 314, 382, 517, 1077, 305, 225]\n",
      "Class 14 | total= 4632 | min=   5 max=1566 mean=463.20 std=469.13 | per-client=[32, 496, 42, 592, 72, 376, 5, 944, 507, 1566]\n",
      "Class 15 | total= 8508 | min= 351 max=2285 mean=850.80 std=557.30 | per-client=[2285, 1350, 626, 638, 529, 438, 713, 1066, 351, 512]\n",
      "Class 16 | total= 2358 | min=  12 max= 514 mean=235.80 std=165.61 | per-client=[12, 514, 425, 273, 23, 138, 71, 381, 209, 312]\n",
      "Class 17 | total= 2964 | min=  50 max= 523 mean=296.40 std=156.74 | per-client=[523, 107, 208, 464, 50, 473, 247, 433, 247, 212]\n",
      "Class 18 | total=11161 | min= 106 max=3191 mean=1116.10 std=1027.45 | per-client=[2075, 106, 2256, 1541, 3191, 160, 758, 500, 155, 419]\n",
      "Class 19 | total= 3498 | min=  58 max= 950 mean=349.80 std=248.51 | per-client=[126, 492, 950, 344, 162, 58, 261, 451, 481, 173]\n",
      "Class 20 | total= 2289 | min=  12 max= 675 mean=228.90 std=187.30 | per-client=[278, 325, 138, 12, 288, 17, 62, 310, 675, 184]\n",
      "Class 21 | total= 4689 | min=  46 max=1422 mean=468.90 std=418.39 | per-client=[46, 829, 406, 313, 1422, 480, 99, 834, 182, 78]\n",
      "Class 22 | total= 8457 | min=  12 max=2953 mean=845.70 std=824.13 | per-client=[355, 656, 2953, 110, 1460, 12, 581, 411, 1256, 663]\n",
      "Class 23 | total= 7656 | min= 196 max=2372 mean=765.60 std=622.36 | per-client=[2372, 1024, 976, 256, 465, 196, 675, 331, 1065, 296]\n",
      "Class 24 | total=23210 | min=  42 max=6499 mean=2321.00 std=2057.35 | per-client=[1980, 4491, 336, 3662, 176, 1678, 42, 6499, 749, 3597]\n",
      "Class 25 | total= 7798 | min=  36 max=2465 mean=779.80 std=659.68 | per-client=[717, 555, 1064, 2465, 650, 262, 1195, 36, 669, 185]\n",
      "Class 26 | total= 2388 | min=  11 max= 640 mean=238.80 std=191.56 | per-client=[14, 61, 177, 142, 304, 11, 447, 248, 640, 344]\n",
      "Class 27 | total= 4698 | min=  24 max=1067 mean=469.80 std=375.66 | per-client=[389, 626, 126, 72, 651, 992, 24, 1067, 711, 40]\n",
      "Class 28 | total=19271 | min=  35 max=8080 mean=1927.10 std=2318.39 | per-client=[155, 35, 966, 2858, 2669, 1324, 277, 170, 2737, 8080]\n",
      "Class 29 | total= 9100 | min= 145 max=1927 mean=910.00 std=559.24 | per-client=[1316, 304, 1512, 1200, 1004, 145, 626, 254, 1927, 812]\n",
      "Class 30 | total=11630 | min=  49 max=2937 mean=1163.00 std=1012.90 | per-client=[687, 2937, 1145, 2868, 49, 1681, 112, 268, 1424, 459]\n",
      "Class 31 | total= 4413 | min=  26 max= 933 mean=441.30 std=289.73 | per-client=[513, 148, 364, 869, 284, 406, 26, 679, 191, 933]\n",
      "Class 32 | total= 4371 | min=  63 max=1323 mean=437.10 std=344.43 | per-client=[181, 576, 428, 1323, 460, 177, 178, 360, 625, 63]\n",
      "Class 33 | total= 2530 | min=   3 max= 559 mean=253.00 std=221.79 | per-client=[502, 476, 559, 62, 495, 54, 36, 292, 3, 51]\n",
      "Class 34 | total= 4416 | min=   6 max=1537 mean=441.60 std=452.68 | per-client=[573, 186, 45, 739, 6, 211, 749, 346, 24, 1537]\n",
      "Class 35 | total= 2477 | min=  18 max= 783 mean=247.70 std=283.39 | per-client=[72, 120, 100, 74, 40, 783, 34, 18, 590, 646]\n",
      "Class 36 | total= 9382 | min=  30 max=2700 mean=938.20 std=918.59 | per-client=[569, 154, 973, 2700, 2670, 30, 948, 306, 542, 490]\n",
      "Class 37 | total= 4785 | min=  26 max=1061 mean=478.50 std=303.50 | per-client=[509, 418, 26, 852, 415, 1061, 359, 151, 728, 266]\n",
      "Class 38 | total= 2621 | min=  19 max= 739 mean=262.10 std=253.53 | per-client=[189, 353, 716, 63, 226, 226, 52, 19, 739, 38]\n",
      "Class 39 | total= 9558 | min=  25 max=4320 mean=955.80 std=1408.20 | per-client=[378, 499, 608, 46, 4320, 96, 3065, 258, 263, 25]\n",
      "Class 40 | total=22967 | min= 213 max=5879 mean=2296.70 std=1892.05 | per-client=[5879, 213, 928, 2380, 4899, 3229, 434, 3390, 840, 775]\n",
      "Class 41 | total= 2365 | min=  10 max= 727 mean=236.50 std=207.72 | per-client=[78, 39, 727, 368, 389, 140, 322, 115, 10, 177]\n",
      "Class 42 | total= 3445 | min=   2 max= 952 mean=344.50 std=308.12 | per-client=[35, 558, 83, 177, 952, 584, 2, 286, 100, 668]\n",
      "Class 43 | total= 8113 | min=   5 max=1986 mean=811.30 std=701.58 | per-client=[1742, 1986, 8, 1424, 407, 432, 96, 5, 772, 1241]\n",
      "Class 44 | total= 2536 | min=  43 max= 936 mean=253.60 std=310.28 | per-client=[173, 73, 89, 789, 193, 51, 47, 43, 936, 142]\n",
      "Class 45 | total= 1795 | min=   2 max= 404 mean=179.50 std=142.46 | per-client=[404, 151, 103, 290, 324, 361, 72, 2, 38, 50]\n",
      "Class 46 | total= 2350 | min=   7 max= 419 mean=235.00 std=135.97 | per-client=[419, 239, 99, 7, 263, 351, 271, 38, 270, 393]\n",
      "Class 47 | total=14255 | min=  30 max=4606 mean=1425.50 std=1240.16 | per-client=[30, 456, 1147, 382, 2065, 4606, 1575, 636, 1562, 1796]\n",
      "Class 48 | total= 2491 | min=   5 max= 681 mean=249.10 std=220.60 | per-client=[681, 598, 24, 248, 335, 69, 269, 130, 132, 5]\n",
      "Class 49 | total=10730 | min= 137 max=2922 mean=1073.00 std=765.48 | per-client=[1652, 968, 2922, 696, 1247, 208, 748, 137, 766, 1386]\n",
      "Class 50 | total= 2595 | min=  36 max= 659 mean=259.50 std=208.76 | per-client=[145, 428, 42, 55, 245, 119, 521, 345, 659, 36]\n",
      "Class 51 | total= 2245 | min=  11 max= 672 mean=224.50 std=236.20 | per-client=[672, 178, 501, 542, 54, 58, 11, 65, 12, 152]\n",
      "Class 52 | total= 2809 | min=  67 max= 656 mean=280.90 std=193.82 | per-client=[251, 243, 654, 226, 165, 208, 146, 67, 656, 193]\n",
      "Class 53 | total=13105 | min=  15 max=4903 mean=1310.50 std=1529.18 | per-client=[15, 321, 1528, 37, 977, 1240, 3362, 4903, 469, 253]\n",
      "Class 54 | total= 2494 | min=  29 max= 644 mean=249.40 std=176.72 | per-client=[29, 448, 314, 644, 109, 96, 152, 151, 258, 293]\n",
      "Class 55 | total=16947 | min= 107 max=3993 mean=1694.70 std=1112.52 | per-client=[750, 107, 551, 2573, 1833, 2700, 1261, 3993, 2015, 1164]\n",
      "Class 56 | total= 2694 | min=   9 max= 904 mean=269.40 std=285.42 | per-client=[86, 904, 514, 14, 65, 9, 463, 61, 465, 113]\n",
      "Class 57 | total= 2701 | min=  69 max=1161 mean=270.10 std=321.37 | per-client=[287, 475, 157, 79, 95, 1161, 74, 81, 69, 223]\n",
      "Class 58 | total= 2567 | min=  39 max= 530 mean=256.70 std=141.87 | per-client=[136, 350, 213, 530, 213, 307, 78, 355, 346, 39]\n",
      "Class 59 | total= 2611 | min=   0 max=1032 mean=261.10 std=277.55 | per-client=[0, 252, 353, 79, 185, 246, 1032, 281, 58, 125]\n",
      "Class 60 | total= 2200 | min=   8 max= 484 mean=220.00 std=149.28 | per-client=[151, 195, 260, 65, 8, 484, 258, 172, 131, 476]\n",
      "Class 61 | total= 2522 | min=   7 max= 613 mean=252.20 std=213.13 | per-client=[177, 592, 49, 403, 613, 49, 68, 252, 7, 312]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Per-class label distribution across clients ===\")\n",
    "\n",
    "for c in range(num_classes):\n",
    "    if c not in client_data:\n",
    "        print(f\"Class {c:2d}: no samples in train split\")\n",
    "        continue\n",
    "\n",
    "    chunks = client_data[c]  # list of arrays, one per client\n",
    "    if chunks is None or len(chunks) == 0:\n",
    "        print(f\"Class {c:2d}: client_data entry is empty\")\n",
    "        continue\n",
    "\n",
    "    counts = np.array([len(chunk) for chunk in chunks], dtype=int)\n",
    "    if counts.size == 0:\n",
    "        print(f\"Class {c:2d}: counts is empty (unexpected)\")\n",
    "        continue\n",
    "\n",
    "    total = int(counts.sum())\n",
    "\n",
    "    print(\n",
    "        f\"Class {c:2d} | total={total:5d} | \"\n",
    "        f\"min={int(counts.min()):4d} max={int(counts.max()):4d} \"\n",
    "        f\"mean={counts.mean():6.2f} std={counts.std():6.2f} | \"\n",
    "        f\"per-client={counts.tolist()}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T12:27:21.332116Z",
     "iopub.status.busy": "2025-11-16T12:27:21.331857Z",
     "iopub.status.idle": "2025-11-16T12:27:22.636759Z",
     "shell.execute_reply": "2025-11-16T12:27:22.636062Z",
     "shell.execute_reply.started": "2025-11-16T12:27:21.332098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAGGCAYAAACqvTJ0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABR6klEQVR4nO3deXxMZ///8fckJIgk1oQQgmhjX6LSWIpbUFVd1N4WoStapLS0JWgrtHctN0ppq6X1LVW0qrUF1UXV0ihF7cutEktICBKS8/vDL3MbSWQmJjPJeD0fj/N4mGuuc87nOpPe87k/c53rmAzDMAQAAAAAAAA4kJuzAwAAAAAAAMDdh6IUAAAAAAAAHI6iFAAAAAAAAByOohQAAAAAAAAcjqIUAAAAAAAAHI6iFAAAAAAAAByOohQAAAAAAAAcjqIUAAAAAAAAHI6iFAAAAAAAAByOohRQQAUFBalfv37ODsOpTpw4oWLFiumXX36xaF+wYIFCQkJUtGhRlSpVyjnB3eLo0aMymUz69NNPnR1KgdWvXz8FBQVZtJlMJo0dO9Yp8ezZs0dFihTR7t27nXJ+AIAlV859Bg4cqHbt2lm0JSQkqGvXripbtqxMJpOmTp3qnOBukd33Nf4nu5xv7NixMplMToupZ8+e6t69u9POD9wJilKAgx06dEjPP/+8qlevrmLFisnHx0fNmzfXtGnTdOXKFWeHZ5N33nlHJpNJdevWzfLehAkTdP/996t8+fIqVqyYatasqaFDh+rMmTNWH3/8+PEKCwtT8+bNzW379u1Tv379VKNGDc2dO1dz5syxy1istXDhwgKTNMI6OX1mtWvXVqdOnTRmzBjHBwUAd5HCnPts3bpVgwcPVp06deTl5aUqVaqoe/fu2r9/v9XHOHLkiD766CO9/vrrFu3Dhg3T6tWrNWrUKC1YsEAPPvigvcPP0T///KOxY8cqLi7OYefEnbndZ/baa6/p66+/1s6dOx0fGHCHijg7AOBusnLlSnXr1k2enp7q06eP6tatq7S0NP38888aMWKE/vrrL4cXWfLqv//9ryZMmCAvL69s39++fbsaNmyonj17ytvbW3v37tXcuXO1cuVKxcXF5bhfpjNnzuizzz7TZ599ZtG+ceNGZWRkaNq0aQoODrbbeKy1cOFC7d69W0OHDrVor1q1qq5cuaKiRYs6PKbC7MqVKypSJH+/inL6zCTphRde0EMPPaRDhw6pRo0a+RoHANyNCnvuM2nSJP3yyy/q1q2b6tevr/j4eM2YMUONGzfWb7/9lu0Pc7eaNm2aqlWrpjZt2li0r1+/Xo8++qiGDx+eX+Hn6J9//tG4ceMUFBSkhg0bWrw3d+5cZWRkODymwuzNN9/UyJEj8/Uct/vMGjVqpCZNmuj999/X/Pnz8zUOwN4oSgEOcuTIEfXs2VNVq1bV+vXrVbFiRfN7gwYN0sGDB7Vy5UonRmib4cOH6/7771d6errOnj2b5f2vv/46S1t4eLi6du2qFStWqGfPnrc9/ueff64iRYqoc+fOFu2nT5+WpFxv2zMMQ1evXlXx4sVzGYl9mEwmFStWzCHncoaMjAylpaXZfYzOvmYREREqXbq0PvvsM40fP96psQCAq3GF3CcqKkoLFy6Uh4eHua1Hjx6qV6+eJk6cqM8///y2+1+7dk1ffPGFXnjhhSzvnT592qplCFJSUnL9Mc+eXP0Htvy4nkWKFMn3H9ly0717d0VHR+uDDz5QyZIlnRoLYAtu3wMc5N1339WlS5f08ccfWyRlmYKDgzVkyJAc909MTNTw4cNVr149lSxZUj4+PurYsWO203SnT5+uOnXqqESJEipdurSaNGmihQsXmt+/ePGihg4dqqCgIHl6esrPz0/t2rXTjh07rBrLpk2btGTJEptvY8tcn+DChQu59l2+fLnCwsIsvlSDgoIUHR0tSSpfvrzFekRBQUF6+OGHtXr1ajVp0kTFixfXhx9+KEmaN2+e/vWvf8nPz0+enp6qXbu2Zs2ale15f/jhB7Vq1Ure3t7y8fHRfffdZ752rVu31sqVK3Xs2DGZTCaZTCbzmHJaU2r9+vVq2bKlvLy8VKpUKT366KPau3evRZ/MdQgOHjyofv36qVSpUvL19VVkZKQuX76c67Vq3bq16tatq+3bt6tZs2YqXry4qlWrptmzZ2fpm5qaqujoaAUHB8vT01OBgYF69dVXlZqaatHPZDJp8ODB+uKLL1SnTh15enpq1apVt43jdtcuJ9mtKXXy5En1799f/v7+8vT0VJ06dfTJJ59Y9Nm4caNMJpMWL16sd955R5UrV1axYsXUtm1bHTx40OLa5PSZSTcS79atW+ubb765bZwAANu5Qu7TrFkzi4KUJNWsWVN16tTJ8n2enZ9//llnz55VRESEue3TTz+VyWSSYRiaOXOm+fvp5vd+/PFHDRw4UH5+fqpcubIk6dixYxo4cKDuvfdeFS9eXGXLllW3bt109OjRLOe9cOGChg0bZh5v5cqV1adPH509e1YbN27UfffdJ0mKjIw0nz8zh8luTamUlBS98sorCgwMlKenp+699179+9//lmEYFv0y84fly5erbt265u/x3HII6X/f7YsWLdLrr7+uChUqyMvLS4888ohOnDiRpf+WLVv04IMPytfXVyVKlFCrVq2yrEOamWPt2bNHvXv3VunSpdWiRYvbxnG7a5eTnNaU+vzzzxUaGqrixYurTJky6tmzZ5axZOZxe/bsUZs2bVSiRAlVqlRJ7777rsW1ud1nJknt2rVTSkqK1q5de9vxAQUNM6UAB1mxYoWqV6+uZs2a5Wn/w4cPa/ny5erWrZuqVaumhIQEffjhh2rVqpX27NmjgIAASTemXL/88svq2rWrhgwZoqtXr+rPP//Uli1b1Lt3b0k3bllasmSJBg8erNq1a+vcuXP6+eeftXfvXjVu3Pi2caSnp+ull17SM888o3r16t22r2EYOnfunK5fv64DBw5o5MiRcnd3V+vWrW+737Vr17R161a9+OKLFu1Tp07V/PnztWzZMs2aNUslS5ZU/fr1ze///fff6tWrl55//nk9++yzuvfeeyVJs2bNUp06dfTII4+oSJEiWrFihQYOHKiMjAwNGjTIvP+nn36q/v37q06dOho1apRKlSqlP/74Q6tWrVLv3r31xhtvKCkpSf/97381ZcoUSbrtL1Hr1q1Tx44dVb16dY0dO1ZXrlzR9OnT1bx5c+3YsSNLwte9e3dVq1ZNMTEx2rFjhz766CP5+flp0qRJt71eknT+/Hk99NBD6t69u3r16qXFixfrxRdflIeHh/r37y/pxmynRx55RD///LOee+451apVS7t27dKUKVO0f/9+LV++3OKY69ev1+LFizV48GCVK1futoue5nbtrJWQkKD777/fnNSWL19eP/zwgwYMGKDk5OQst+BNnDhRbm5uGj58uJKSkvTuu+/qySef1JYtWyTJqs8sNDRU33zzjZKTk+Xj42N1rACA23OV3OdWhmEoISFBderUybXvr7/+KpPJpEaNGpnbHnjgAS1YsEBPP/202rVrpz59+mTZb+DAgSpfvrzGjBmjlJQUSTfWt/r111/Vs2dPVa5cWUePHtWsWbPUunVr7dmzRyVKlJAkXbp0SS1bttTevXvVv39/NW7cWGfPntW3336r//73v6pVq5bGjx+vMWPG6LnnnlPLli0lKcfPyTAMPfLII9qwYYMGDBighg0bavXq1RoxYoROnjxp/n7N9PPPP2vp0qUaOHCgvL299Z///EdPPPGEjh8/rrJly+Z6zTLXLH3ttdd0+vRpTZ06VREREYqLizPPgF+/fr06duyo0NBQRUdHy83Nzfwj5E8//aSmTZtaHLNbt26qWbOmJkyYkKWQdrPcrl25cuVyjf/mcYwePVrdu3fXM888ozNnzmj69Ol64IEH9Mcff1jMkjt//rwefPBBdenSRd27d9eSJUv02muvqV69eurYsaNVn1nt2rVVvHhx/fLLL3r88cetjhNwOgNAvktKSjIkGY8++qjV+1StWtXo27ev+fXVq1eN9PR0iz5HjhwxPD09jfHjx5vbHn30UaNOnTq3Pbavr68xaNAgq2O52YwZMwxfX1/j9OnThmEYRqtWrXI836lTpwxJ5q1y5crGokWLcj3HwYMHDUnG9OnTs7wXHR1tSDLOnDlj0V61alVDkrFq1aos+1y+fDlLW4cOHYzq1aubX1+4cMHw9vY2wsLCjCtXrlj0zcjIMP+7U6dORtWqVbMc78iRI4YkY968eea2hg0bGn5+fsa5c+fMbTt37jTc3NyMPn36ZBlT//79LY75+OOPG2XLls1yrlu1atXKkGS8//775rbU1FTz+dPS0gzDMIwFCxYYbm5uxk8//WSx/+zZsw1Jxi+//GJuk2S4ubkZf/31V67nt/ba9e3bN8u1k2RER0ebXw8YMMCoWLGicfbsWYt+PXv2NHx9fc2f5YYNGwxJRq1atYzU1FRzv2nTphmSjF27dpnbcvrMMi1cuNCQZGzZsiXXsQIArONKuc+tFixYYEgyPv7441z7PvXUUzl+l0vKEtO8efMMSUaLFi2M69evW7yXXT6zefNmQ5Ixf/58c9uYMWMMScbSpUuz9M/8Xt66dWuWvCXTrd/Xy5cvNyQZb7/9tkW/rl27GiaTyTh48KDFmDw8PCzadu7cmWNed7PM7/ZKlSoZycnJ5vbFixcbkoxp06aZx1CzZk2jQ4cOFnnG5cuXjWrVqhnt2rUzt2XmWL169brtuTNZc+2yy/kyz5Pp6NGjhru7u/HOO+9YHGPXrl1GkSJFLNoz87ibP8PU1FSjQoUKxhNPPGFuu91nlumee+4xOnbsaNVYgYKC2/cAB0hOTpYkeXt75/kYnp6ecnO78Z9senq6zp07p5IlS+ree++1mHpeqlQp/fe//9XWrVtzPFapUqW0ZcsW/fPPPzbFcO7cOY0ZM0ajR49W+fLlc+1fpkwZrV27VitWrND48eNVrlw5Xbp0yarzSFLp0qVtiq9atWrq0KFDlvab15VKSkrS2bNn1apVKx0+fFhJSUmSpLVr1+rixYsaOXJklnWO8vKI31OnTikuLk79+vVTmTJlzO3169dXu3bt9P3332fZ59b1Jlq2bKlz586Z/35up0iRInr++efNrz08PPT888/r9OnT2r59uyTpq6++Uq1atRQSEqKzZ8+at3/961+SpA0bNlgcs1WrVqpdu3au57bXtTMMQ19//bU6d+4swzAsYuzQoYOSkpKy3GYRGRlpcVtF5i+Hhw8ftvq8mX9nt5uWDwCwjavkPrfat2+fBg0apPDwcPXt2zfX/ufOnbM5n5GkZ599Vu7u7hZtN+cz165d07lz5xQcHKxSpUpZXI+vv/5aDRo0yHa2TF5ymu+//17u7u56+eWXLdpfeeUVGYahH374waI9IiLC4uEh9evXl4+Pj9XfzX369LH4u+natasqVqxozp3i4uJ04MAB9e7dW+fOnTPnCikpKWrbtq02bdqUZaH27Nb0yo69rt3SpUuVkZGh7t27W+QzFSpUUM2aNbPkXCVLltRTTz1lfu3h4aGmTZvalM9IN3Ia8hkUNty+BzhA5i1BFy9ezPMxMp8498EHH+jIkSNKT083v3fzVOjXXntN69atU9OmTRUcHKz27durd+/eat68ubnPu+++q759+yowMFChoaF66KGH1KdPH1WvXv22Mbz55psqU6aMXnrpJati9vDwMK+h8PDDD6tt27Zq3ry5/Pz89PDDD+e6v3Gb6dXZqVatWrbtv/zyi6Kjo7V58+YsazQlJSXJ19dXhw4dkiSrnqJjjWPHjkmS+RbCm9WqVUurV6/OstBmlSpVLPplJrHnz5/P9baygICALIt23nPPPZJurHd1//3368CBA9q7d2+OBcXMReQz5XQ9b2Wva3fmzBlduHBBc+bMyfFJTLfGeLtrZq3Mv7O8JOoAgOy5Su5zs/j4eHXq1Em+vr5asmRJlqJRTmzNZ6Tsv4OvXLmimJgYzZs3TydPnrQ4buaPbNKN7+UnnnjC5nPm5NixYwoICMhSYKxVq5b5/Zvd+t0s3fh+tva7uWbNmhavTSaTgoODzWtnHThwQJJuWxRMSkqyKAbaktPY49odOHBAhmFkGUumWxeTr1y5cpY8pHTp0vrzzz9tOq9hGOQzKHQoSgEO4OPjo4CAAO3evTvPx5gwYYJGjx6t/v3766233lKZMmXk5uamoUOHWvwaVKtWLf3999/67rvvtGrVKn399df64IMPNGbMGI0bN07SjbWLWrZsqWXLlmnNmjV67733NGnSJC1dulQdO3bM9vwHDhzQnDlzNHXqVItfGa9evapr167p6NGj8vHxsZgVdKtmzZqpYsWK+uKLL25blMpMNG0pLEjK9kl7hw4dUtu2bRUSEqLJkycrMDBQHh4e+v777zVlypQC9cjjnJLbvCSz2cnIyFC9evU0efLkbN8PDAy0eO2oJxdmyvwsnnrqqRwTzZvXEJPsc80y/85sWScCAHB7rpD73CwpKUkdO3bUhQsX9NNPP5nXs8pN2bJlbc5npOy/g1966SXNmzdPQ4cOVXh4uHx9fWUymdSzZ8+7Lp+RpPfee08NGzbMts+t60c6I6cxmUz64Ycfsr0et8Znr2t2/vz5HAthQEFFUQpwkIcfflhz5szR5s2bFR4ebvP+S5YsUZs2bfTxxx9btF+4cCHL/5n28vJSjx491KNHD6WlpalLly565513NGrUKPPtVRUrVtTAgQM1cOBAnT59Wo0bN9Y777yTY2J28uRJZWRk6OWXX84yfVu68QvUkCFDcn0i39WrVy1+zctOlSpVVLx4cR05cuS2/ayxYsUKpaam6ttvv7X45e7WadOZ08x3796t4ODgHI9n7a9PVatWlXRj8fVb7du3T+XKlbPr44j/+eefLDOv9u/fL+l/Tz2sUaOGdu7cqbZt29r1VzRrr11uypcvL29vb6Wnp1s8pehO5TbWI0eOyM3NzTyzDABgH4U998l09epVde7cWfv379e6deusurU9U0hIiL744gvzzOw7sWTJEvXt21fvv/++RWy3PtW4Ro0auRYDbckDqlatqnXr1unixYsWs6X27dtnft+eMmdCZTIMQwcPHjT/MJWZd/j4+Ng1X8g89p0UUm8+jmEYqlatmt3yi9w+s+vXr+vEiRN65JFH7HI+wFFYUwpwkFdffVVeXl565plnlJCQkOX9Q4cOadq0aTnu7+7unuXXkq+++konT560aMtcjymTh4eHateuLcMwdO3aNaWnp2cpCvn5+SkgIECpqak5nr9u3bpatmxZlq1OnTqqUqWKli1bpgEDBki68djgW2+Tk27cp3/+/Hk1adIkx/NIN6Y0N2nSRNu2bbttP2tk/vJ06xT3efPmWfRr3769vL29FRMTo6tXr1q8d/O+Xl5euRbVpBuJb8OGDfXZZ59ZJIu7d+/WmjVr9NBDD+VlODm6fv26PvzwQ/PrtLQ0ffjhhypfvrxCQ0Ml3fiV+OTJk5o7d26W/a9cuWJ+uo+trL12uXF3d9cTTzyhr7/+OtuE8MyZM3mKL7fPbPv27apTp84d/58FAIClwp77SDfWsurRo4c2b96sr776yubiWnh4uAzDMK/veCeyux7Tp0+3uK1Rkp544gnt3LlTy5Yty3KMzP0zf8S6taCVnYceekjp6emaMWOGRfuUKVNkMpmsmmlmi/nz51vc9rlkyRKdOnXKfJ7Q0FDVqFFD//73v7NdqzSv+YJk3bWzRpcuXeTu7q5x48Zl2c/4/0+ntlVun9mePXt09erVPD/tEnAWZkoBDlKjRg0tXLhQPXr0UK1atdSnTx/VrVtXaWlp+vXXX/XVV1+pX79+Oe7/8MMPa/z48YqMjFSzZs20a9cuffHFF1nWQmjfvr0qVKig5s2by9/fX3v37tWMGTPUqVMneXt768KFC6pcubK6du2qBg0aqGTJklq3bp22bt1q8cvbrcqVK6fHHnssS3vmzKib3ztw4IAiIiLUo0cPhYSEyM3NTdu2bdPnn3+uoKAgDRkyJNfr9eijj+qNN95QcnJyrusp3U779u3l4eGhzp076/nnn9elS5c0d+5c+fn56dSpU+Z+Pj4+mjJlip555hndd9996t27t0qXLq2dO3fq8uXL+uyzzyTdSIQWLVqkqKgo3XfffSpZsqQ6d+6c7bnfe+89dezYUeHh4RowYICuXLmi6dOny9fXV2PHjs3zmLITEBCgSZMm6ejRo7rnnnu0aNEixcXFac6cOeZ1C55++mktXrxYL7zwgjZs2KDmzZsrPT1d+/bt0+LFi7V69epcC4bZsfbaWWPixInasGGDwsLC9Oyzz6p27dpKTEzUjh07tG7dOiUmJtoc3+0+s2vXrunHH3/UwIEDbT4uAOD2CnvuI91YzPvbb79V586dlZiYqM8//9zi/ZsXp85OixYtVLZsWa1bt878YJG8evjhh7VgwQL5+vqqdu3a2rx5s9atW2exvpYkjRgxQkuWLFG3bt3Uv39/hYaGKjExUd9++61mz56tBg0aqEaNGipVqpRmz54tb29veXl5KSwsLNu1lzp37qw2bdrojTfe0NGjR9WgQQOtWbNG33zzjYYOHWqxqLk9lClTRi1atFBkZKQSEhI0depUBQcH69lnn5Ukubm56aOPPlLHjh1Vp04dRUZGqlKlSjp58qQ2bNggHx8frVixIk/ntubaWaNGjRp6++23NWrUKB09elSPPfaYvL29deTIES1btkzPPfechg8fblNsuX1ma9euVYkSJdSuXTubxw04lWMe8gcg0/79+41nn33WCAoKMjw8PAxvb2+jefPmxvTp042rV6+a+2X3WORXXnnFqFixolG8eHGjefPmxubNm41WrVoZrVq1Mvf78MMPjQceeMAoW7as4enpadSoUcMYMWKEkZSUZBjGjUfMjhgxwmjQoIHh7e1teHl5GQ0aNDA++OCDPI2nVatWWR7DfObMGeO5554zQkJCDC8vL8PDw8OoWbOmMXToUOPMmTNWHTchIcEoUqSIsWDBAov2zEfu3nqcqlWrGp06dcr2WN9++61Rv359o1ixYkZQUJAxadIk45NPPjEkGUeOHMnSt1mzZkbx4sUNHx8fo2nTpsb//d//md+/dOmS0bt3b6NUqVKGJPMjk7N7PLBhGMa6deuM5s2bm4/XuXNnY8+ePVaNKfOx0LfGeKvMz2Dbtm1GeHi4UaxYMaNq1arGjBkzsvRNS0szJk2aZNSpU8fw9PQ0SpcubYSGhhrjxo0z/40YRvaPqc5Nbtfu1kdMZ54nOjraoi0hIcEYNGiQERgYaBQtWtSoUKGC0bZtW2POnDnmPpmPjf7qq68s9s3uc8jpMzMMw/jhhx8MScaBAwdsGisAwHqFOfdp1aqVISnHzRovv/yyERwcnKU9u+/azO/+rVu3Zul//vx5IzIy0ihXrpxRsmRJo0OHDsa+ffuyXDfDMIxz584ZgwcPNipVqmR4eHgYlStXNvr27WucPXvW3Oebb74xateubRQpUsTiuzO77+uLFy8aw4YNMwICAoyiRYsaNWvWNN577z0jIyMj1zEZRtbPNjuZ3+3/93//Z4waNcrw8/MzihcvbnTq1Mk4duxYlv5//PGH0aVLF/PnXrVqVaN79+5GbGysuU9OOdbt5Hbtsss1Ms9zq6+//tpo0aKF4eXlZXh5eRkhISHGoEGDjL///tvcJ7tc2jCy/xxy+swMwzDCwsKMp556yupxAgWFyTDstOIcANjZgAEDtH//fv3000/ODqVAa926tc6ePWuXNRDuNo899phMJlO20/QBALCHw4cPKyQkRD/88IPatm3r7HAKrI0bN6pNmzb66quv1LVrV2eHU6jExcWpcePG2rFjR46LvwMFFWtKASiwoqOjtXXrVv3yyy/ODgUuaO/evfruu+/01ltvOTsUAIALq169ugYMGKCJEyc6OxS4qIkTJ6pr164UpFAosaYUgAKrSpUqWRbOBuylVq1aun79urPDAADcBWbNmuXsEODCvvzyS2eHAOQZM6UAAAAAAADgcE4tSm3atEmdO3dWQECATCaTli9fnus+GzduVOPGjeXp6ang4GB9+umn+R4nABRkGzduZD0pAABQqLVu3VqGYbCeFHCXcWpRKiUlRQ0aNNDMmTOt6n/kyBF16tRJbdq0UVxcnIYOHapnnnlGq1evzudIAQAAAAAAYE8F5ul7mU8/euyxx3Ls89prr2nlypUWMwJ69uypCxcuaNWqVQ6IEgAAAAAAAPZQqBY637x5syIiIizaOnTooKFDh+a4T2pqqlJTU82vMzIylJiYqLJly8pkMuVXqAAAALkyDEMXL15UQECA3Nxsn8BOngMAAAoia3OcQlWUio+Pl7+/v0Wbv7+/kpOTdeXKFRUvXjzLPjExMRo3bpyjQgQAALDZiRMnVLlyZZv3I88BAAAFWW45TqEqSuXFqFGjFBUVZX6dlJSkKlWq6MSJE/Lx8XFiZAAA4G6XnJyswMBAeXt752l/8hwAAFAQWZvjFKqiVIUKFZSQkGDRlpCQIB8fn2xnSUmSp6enPD09s7T7+PiQrAEAgAIhr7fakecAAICCLLccx6lP37NVeHi4YmNjLdrWrl2r8PBwJ0UEAAAAAACAvHBqUerSpUuKi4tTXFycJOnIkSOKi4vT8ePHJd2Ykt6nTx9z/xdeeEGHDx/Wq6++qn379umDDz7Q4sWLNWzYMGeEDwAAAAAAgDxyalFq27ZtatSokRo1aiRJioqKUqNGjTRmzBhJ0qlTp8wFKkmqVq2aVq5cqbVr16pBgwZ6//339dFHH6lDhw5OiR8AAAAAAAB5YzIMw3B2EI6UnJwsX19fJSUlsdYCAABwKnvnJeQ5AACgILA2JylUa0oBAAAAAADANVCUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMNRlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMM5vSg1c+ZMBQUFqVixYgoLC9Pvv/9+2/5Tp07Vvffeq+LFiyswMFDDhg3T1atXHRQtAAAAAAAA7MGpRalFixYpKipK0dHR2rFjhxo0aKAOHTro9OnT2fZfuHChRo4cqejoaO3du1cff/yxFi1apNdff93BkQMAAAAAAOBOOLUoNXnyZD377LOKjIxU7dq1NXv2bJUoUUKffPJJtv1//fVXNW/eXL1791ZQUJDat2+vXr165Tq7CgAAAAAAAAWL04pSaWlp2r59uyIiIv4XjJubIiIitHnz5mz3adasmbZv324uQh0+fFjff/+9HnrooRzPk5qaquTkZIsNAADAFZDnAACAwsxpRamzZ88qPT1d/v7+Fu3+/v6Kj4/Pdp/evXtr/PjxatGihYoWLaoaNWqodevWt719LyYmRr6+vuYtMDDQruMAAABwFvIcAABQmDl9oXNbbNy4URMmTNAHH3ygHTt2aOnSpVq5cqXeeuutHPcZNWqUkpKSzNuJEyccGDEAAED+Ic8BAACFWRFnnbhcuXJyd3dXQkKCRXtCQoIqVKiQ7T6jR4/W008/rWeeeUaSVK9ePaWkpOi5557TG2+8ITe3rDU2T09PeXp62n8AAAAATkaeAwAACjOnzZTy8PBQaGioYmNjzW0ZGRmKjY1VeHh4tvtcvnw5S+HJ3d1dkmQYRv4FCwAAAAAAALty2kwpSYqKilLfvn3VpEkTNW3aVFOnTlVKSooiIyMlSX369FGlSpUUExMjSercubMmT56sRo0aKSwsTAcPHtTo0aPVuXNnc3EKAAAAAAAABZ9Ti1I9evTQmTNnNGbMGMXHx6thw4ZatWqVefHz48ePW8yMevPNN2UymfTmm2/q5MmTKl++vDp37qx33nnHWUMAAAAAAABAHpiMu+y+t+TkZPn6+iopKUk+Pj7ODgcAANzF7J2XkOcAAICCwNqcpFA9fQ8AAAAAAACugaIUAAAAAAAAHI6iFAAAAAAAAByOohQAAAAAAAAcjqIUAAAAAAAAHI6iFAAAAAAAAByOohQAAAAAAAAcjqIUAAAAAAAAHI6iFAAAAAAAAByOohQAAAAAAAAcjqIUAAAAAAAAHI6iFAAAAAAAAByOohQAAAAAAAAcjqIUAAAAAAAAHO6Oi1Lp6emKi4vT+fPn7REPAAAAAAAA7gI2F6WGDh2qjz/+WNKNglSrVq3UuHFjBQYGauPGjfaODwAAAAAAAC7I5qLUkiVL1KBBA0nSihUrdOTIEe3bt0/Dhg3TG2+8YfcAAQAAAAAA4HpsLkqdPXtWFSpUkCR9//336tatm+655x71799fu3btsnuAAAAAAAAAcD02F6X8/f21Z88epaena9WqVWrXrp0k6fLly3J3d7d7gAAAAAAAAHA9RWzdITIyUt27d1fFihVlMpkUEREhSdqyZYtCQkLsHiAAAAAAAABcj81FqbFjx6pu3bo6ceKEunXrJk9PT0mSu7u7Ro4cafcAAQAAAAAA4HpsLkpJUteuXS1eX7hwQX379rVLQAAAAAAAAHB9Nq8pNWnSJC1atMj8unv37ipbtqwqV66sP//8067BAQAAAAAAwDXZXJSaPXu2AgMDJUlr167V2rVr9cMPP+jBBx/U8OHD7R4gAAAAAAAAXI/Nt+/Fx8ebi1Lfffedunfvrvbt2ysoKEhhYWF2DxAAAAAAAACux+aZUqVLl9aJEyckSatWrTI/fc8wDKWnp9s3OgAAAAAAALgkm2dKdenSRb1791bNmjV17tw5dezYUZL0xx9/KDg42O4BAgAAAAAAwPXYXJSaMmWKgoKCdOLECb377rsqWbKkJOnUqVMaOHCg3QMEAAAAAACA6zEZhmE4OwhHSk5Olq+vr5KSkuTj4+PscAAAwF3M3nkJeQ4AACgIrM1JbJ4plWnPnj06fvy40tLSLNofeeSRvB4SAAAAAAAAdwmbi1KHDx/W448/rl27dslkMilzopXJZJIkFjsHAAAAAABArmx++t6QIUNUrVo1nT59WiVKlNBff/2lTZs2qUmTJtq4cWM+hAgAAAAAAABXY/NMqc2bN2v9+vUqV66c3Nzc5ObmphYtWigmJkYvv/yy/vjjj/yIEwAAAAAAAC7E5plS6enp8vb2liSVK1dO//zzjySpatWq+vvvv+0bHQAAAAAAAFySzTOl6tatq507d6patWoKCwvTu+++Kw8PD82ZM0fVq1fPjxgBAAAAAADgYmwuSr355ptKSUmRJI0fP14PP/ywWrZsqbJly2rRokV2DxAAAAAAAACux+aiVIcOHcz/Dg4O1r59+5SYmKjSpUubn8AHAAAAAAAA3I7NRanslClTxh6HAQAAAAAAwF3CqqJUly5drD7g0qVL8xwMAAAAAAAA7g5WFaV8fX3zOw4AAAAAAADcRawqSs2bNy+/4wAAAAAAAMBdxM3WHY4cOaIDBw5kaT9w4ICOHj1qj5gAAAAAAADg4mwuSvXr10+//vprlvYtW7aoX79+9ogJAAAAAAAALs7motQff/yh5s2bZ2m///77FRcXZ4+YAAAAAAAA4OJsLkqZTCZdvHgxS3tSUpLS09PtEhQAAAAAAABcm81FqQceeEAxMTEWBaj09HTFxMSoRYsWdg0OAAAAAAAArsmqp+/dbNKkSXrggQd07733qmXLlpKkn376ScnJyVq/fr3dAwQAAAAAAIDrsXmmVO3atfXnn3+qe/fuOn36tC5evKg+ffpo3759qlu3bn7ECAAAAAAAABdj80wpSQoICNCECRPsHQsAAAAAAADuEjbPlAIAAAAAAADulNOLUjNnzlRQUJCKFSumsLAw/f7777ftf+HCBQ0aNEgVK1aUp6en7rnnHn3//fcOihYAAAAAAAD2kKfb9+xl0aJFioqK0uzZsxUWFqapU6eqQ4cO+vvvv+Xn55elf1pamtq1ayc/Pz8tWbJElSpV0rFjx1SqVCnHBw8AAAAAAIA8c2pRavLkyXr22WcVGRkpSZo9e7ZWrlypTz75RCNHjszS/5NPPlFiYqJ+/fVXFS1aVJIUFBTkyJABAAAAAABgBzbfvnflyhVdvnzZ/PrYsWOaOnWq1qxZY9Nx0tLStH37dkVERPwvGDc3RUREaPPmzdnu8+233yo8PFyDBg2Sv7+/6tatqwkTJig9PT3H86Smpio5OdliAwAAcAXkOQAAoDCzuSj16KOPav78+ZJurO8UFham999/X48++qhmzZpl9XHOnj2r9PR0+fv7W7T7+/srPj4+230OHz6sJUuWKD09Xd9//71Gjx6t999/X2+//XaO54mJiZGvr695CwwMtDpGAACAgow8BwAAFGY2F6V27Nihli1bSpKWLFkif39/HTt2TPPnz9d//vMfuwd4s4yMDPn5+WnOnDkKDQ1Vjx499MYbb2j27Nk57jNq1CglJSWZtxMnTuRrjAAAAI5CngMAAAozm9eUunz5sry9vSVJa9asUZcuXeTm5qb7779fx44ds/o45cqVk7u7uxISEizaExISVKFChWz3qVixoooWLSp3d3dzW61atRQfH6+0tDR5eHhk2cfT01Oenp5WxwUAAFBYkOcAAIDCzOaZUsHBwVq+fLlOnDih1atXq3379pKk06dPy8fHx+rjeHh4KDQ0VLGxsea2jIwMxcbGKjw8PNt9mjdvroMHDyojI8Pctn//flWsWDHbghQAAAAAAAAKJpuLUmPGjNHw4cMVFBSksLAwcwFpzZo1atSokU3HioqK0ty5c/XZZ59p7969evHFF5WSkmJ+Gl+fPn00atQoc/8XX3xRiYmJGjJkiPbv36+VK1dqwoQJGjRokK3DAAAAAAAAgBPZfPte165d1aJFC506dUoNGjQwt7dt21aPP/64Tcfq0aOHzpw5ozFjxig+Pl4NGzbUqlWrzIufHz9+XG5u/6ubBQYGavXq1Ro2bJjq16+vSpUqaciQIXrttddsHQYAAAAAAACcyGQYhuHsIBwpOTlZvr6+SkpKsul2QwAAAHuzd15CngMAAAoCa3MSq2ZKdenSRZ9++ql8fHzUpUuX2/ZdunSpbZECAAAAAADgrmNVUcrX11cmk8n8bwAAAAAAAOBOWFWUmjdvXrb/BgAAAAAAAPLC5qfvAQAAAAAAAHfK5qJUQkKCnn76aQUEBKhIkSJyd3e32AAAAAAAAIDcWHX73s369eun48ePa/To0apYsaJ5rSkAAAAAAADAWjYXpX7++Wf99NNPatiwYT6EAwAAAAAAgLuBzbfvBQYGyjCM/IgFAAAAAAAAdwmbi1JTp07VyJEjdfTo0XwIBwAAAAAAAHcDq27fK126tMXaUSkpKapRo4ZKlCihokWLWvRNTEy0b4QAAAAAAABwOVYVpaZOnZrPYQAAAAAAAOBuYlVRqm/fvvkdBwAAAAAAAO4iNq8p5e7urtOnT2dpP3funNzd3e0SFAAAAAAAAFybzUWpnJ68l5qaKg8PjzsOCAAAAAAAAK7Pqtv3JOk///mPJMlkMumjjz5SyZIlze+lp6dr06ZNCgkJsX+EAAAAAAAAcDlWF6WmTJki6cZMqdmzZ1vcqufh4aGgoCDNnj3b/hECAAAAAADA5VhdlDpy5IgkqU2bNlq6dKlKly6db0EBAAAAAADAtVldlMq0YcOG/IgDAAAAAAAAdxGbFzoHAAAAAAAA7hRFKQAAAAAAADgcRSkAAAAAAAA4nFVFqS5duig5OVmSNH/+fKWmpuZrUAAAAAAAAHBtVhWlvvvuO6WkpEiSIiMjlZSUlK9BAQAAAAAAwLVZ9fS9kJAQjRo1Sm3atJFhGFq8eLF8fHyy7dunTx+7BggAAAAAAADXYzIMw8it06+//qqoqCgdOnRIiYmJ8vb2lslkynowk0mJiYn5Eqi9JCcny9fXV0lJSTkW1gAAABzB3nkJeQ4AACgIrM1JrJop1axZM/3222+SJDc3N+3fv19+fn72iRQAAAAAAAB3HZufvnfkyBGVL18+P2IBAAAAAADAXcKqmVI3q1q1qi5cuKCPP/5Ye/fulSTVrl1bAwYMkK+vr90DBAAAAAAAgOuxeabUtm3bVKNGDU2ZMkWJiYlKTEzUlClTVKNGDe3YsSM/YgQAAAAAAICLsXmm1LBhw/TII49o7ty5KlLkxu7Xr1/XM888o6FDh2rTpk12DxIAAAAAAACuxeai1LZt2ywKUpJUpEgRvfrqq2rSpIldgwMAAAAAAIBrsvn2PR8fHx0/fjxL+4kTJ+Tt7W2XoAAAAAAAAODabC5K9ejRQwMGDNCiRYt04sQJnThxQl9++aWeeeYZ9erVKz9iBAAAAAAAgIux+fa9f//73zKZTOrTp4+uX78uSSpatKhefPFFTZw40e4BAgAAAAAAwPXYXJTy8PDQtGnTFBMTo0OHDkmSatSooRIlStg9OAAAAAAAALgmm4tSmUqUKKF69erZMxYAAAAAAADcJWxeUwoAAAAAAAC4UxSlAAAAAAAA4HAUpQAAAAAAAOBwFKUAAAAAAADgcHla6PzAgQPasGGDTp8+rYyMDIv3xowZY5fAAAAAAAAA4LpsLkrNnTtXL774osqVK6cKFSrIZDKZ3zOZTBSlAAAAAAAAkCubi1Jvv/223nnnHb322mv5EQ8AAAAAAADuAjavKXX+/Hl169YtP2IBAAAAAADAXcLmolS3bt20Zs2a/IgFAAAAAAAAdwmbb98LDg7W6NGj9dtvv6levXoqWrSoxfsvv/yy3YIDAAAAAACAazIZhmHYskO1atVyPpjJpMOHD99xUPkpOTlZvr6+SkpKko+Pj7PDAQAAdzF75yXkOQAAoCCwNiexeabUkSNH7igwAAAAAAAAwOY1pW5mGIZsnGgFAAAAAAAA5K0oNX/+fNWrV0/FixdX8eLFVb9+fS1YsCDPQcycOVNBQUEqVqyYwsLC9Pvvv1u135dffimTyaTHHnssz+cGAAAAAACA49lclJo8ebJefPFFPfTQQ1q8eLEWL16sBx98UC+88IKmTJlicwCLFi1SVFSUoqOjtWPHDjVo0EAdOnTQ6dOnb7vf0aNHNXz4cLVs2dLmcwIAAAAAAMC58rTQ+bhx49SnTx+L9s8++0xjx461ec2psLAw3XfffZoxY4YkKSMjQ4GBgXrppZc0cuTIbPdJT0/XAw88oP79++unn37ShQsXtHz5cqvOxwKgAACgoGChcwAA4IqszUlsnil16tQpNWvWLEt7s2bNdOrUKZuOlZaWpu3btysiIuJ/Abm5KSIiQps3b85xv/Hjx8vPz08DBgyw6XwAAAAAAAAoGGwuSgUHB2vx4sVZ2hctWqSaNWvadKyzZ88qPT1d/v7+Fu3+/v6Kj4/Pdp+ff/5ZH3/8sebOnWvVOVJTU5WcnGyxAQAAuALyHAAAUJgVsXWHcePGqUePHtq0aZOaN28uSfrll18UGxubbbHKni5evKinn35ac+fOVbly5azaJyYmRuPGjcvXuAAAAJyBPAcAABRmNq8pJUnbt2/XlClTtHfvXklSrVq19Morr6hRo0Y2HSctLU0lSpTQkiVLLJ6g17dvX124cEHffPONRf+4uDg1atRI7u7u5raMjAxJN277+/vvv1WjRg2LfVJTU5Wammp+nZycrMDAQNZaAAAATnena0CR5wAAgILI2hzH5plSkhQaGqrPP/88z8Fl8vDwUGhoqGJjY81FqYyMDMXGxmrw4MFZ+oeEhGjXrl0WbW+++aYuXryoadOmKTAwMMs+np6e8vT0vONYAQAAChryHAAAUJhZVZRKTk42V7ZyW6vA1l/loqKi1LdvXzVp0kRNmzbV1KlTlZKSosjISElSnz59VKlSJcXExKhYsWKqW7euxf6lSpWSpCztAAAAAAAAKLisKkqVLl1ap06dkp+fn0qVKiWTyZSlj2EYMplMSk9PtymAHj166MyZMxozZozi4+PVsGFDrVq1yrz4+fHjx+XmZvN67AAAAAAAACjArFpT6scff1Tz5s1VpEgR/fjjj7ft26pVK7sFlx/udO0GAAAAe7F3XkKeAwAACgK7ril1c6GpWrVqCgwMzDJbyjAMnThxIo/hAgAAAAAA4G5i831x1apV05kzZ7K0JyYmqlq1anYJCgAAAAAAAK7N5qJU5tpRt7p06ZKKFStml6AAAAAAAADg2qy6fU+68ZQ8STKZTBo9erRKlChhfi89PV1btmxRw4YN7R4gAAAAAAAAXI/VRak//vhD0o2ZUrt27ZKHh4f5PQ8PDzVo0EDDhw+3f4QAAAAAAABwOVYXpTZs2CBJioyM1LRp03iiCwAAAAAAAPLM5jWlpk6dquvXr2dpT0xMVHJysl2CAgAAAAAAgGuzuSjVs2dPffnll1naFy9erJ49e9olKAAAAAAAALg2m4tSW7ZsUZs2bbK0t27dWlu2bLFLUAAAAAAAAHBtNhelUlNTs71979q1a7py5YpdggIAAAAAAIBrs7ko1bRpU82ZMydL++zZsxUaGmqXoAAAAAAAAODarH76Xqa3335bERER2rlzp9q2bStJio2N1datW7VmzRq7BwgAAAAAAADXY/NMqebNm2vz5s0KDAzU4sWLtWLFCgUHB+vPP/9Uy5Yt8yNGAAAAAAAAuBibZ0pJUsOGDfXFF1/YOxYAAAAAAADcJfJUlMp09epVpaWlWbT5+PjcUUAAAAAAAABwfTbfvnf58mUNHjxYfn5+8vLyUunSpS02AAAAAAAAIDc2F6VGjBih9evXa9asWfL09NRHH32kcePGKSAgQPPnz8+PGAEAAAAAAOBibL59b8WKFZo/f75at26tyMhItWzZUsHBwapataq++OILPfnkk/kRJwAAAAAAAFyIzTOlEhMTVb16dUk31o9KTEyUJLVo0UKbNm2yb3QAAAAAAABwSTYXpapXr64jR45IkkJCQrR48WJJN2ZQlSpVyq7BAQAAAAAAwDXZXJSKjIzUzp07JUkjR47UzJkzVaxYMQ0bNkwjRoywe4AAAAAAAABwPTavKTVs2DDzvyMiIrRv3z5t375dwcHBql+/vl2DAwAAAAAAri9o5Epnh5CroxM7OTsEl2PTTKlr166pbdu2OnDggLmtatWq6tKlCwUpAAAAAAAAWM2molTRokX1559/5lcsAAAAAAAAuEvYvKbUU089pY8//jg/YgEAAAAAAMBdwuY1pa5fv65PPvlE69atU2hoqLy8vCzenzx5st2CAwAAAAAAgGuyuSi1e/duNW7cWJK0f/9+i/dMJpN9ogIAAAAAAIBLs7oodfjwYVWrVk0bNmzIz3gAAAAAAABwF7B6TamaNWvqzJkz5tc9evRQQkJCvgQFAAAAAAAA12Z1UcowDIvX33//vVJSUuweEAAAAAAAAFyfzU/fAwAAAAAAAO6U1UUpk8mUZSFzFjYHAAAAAABAXli90LlhGOrXr588PT0lSVevXtULL7wgLy8vi35Lly61b4QAAAAAAABwOVYXpfr27Wvx+qmnnrJ7MAAAAAAAALg7WF2UmjdvXn7GAQAAAAAAgLsIC50DAAAAAADA4ShKAQAAAAAAwOEoSgEAAAAAAMDhKEoBAAAAAADA4ShKAQAAAAAAwOEoSgEAAAAAAMDhijg7ABR8QSNXOjuEXB2d2MnZIQAAAAAAABswUwoAAAAAAAAOR1EKAAAAAAAADkdRCgAAAAAAAA5HUQoAAAAAAAAOx0LnAAAAAAAUQjyUCoUdM6UAAAAAAADgcBSlAAAAAAAA4HAUpQAAAAAAAOBwBWJNqZkzZ+q9995TfHy8GjRooOnTp6tp06bZ9p07d67mz5+v3bt3S5JCQ0M1YcKEHPs7C/f2AgAAAAAA5MzpM6UWLVqkqKgoRUdHa8eOHWrQoIE6dOig06dPZ9t/48aN6tWrlzZs2KDNmzcrMDBQ7du318mTJx0cOQAAAAAAAPLK6UWpyZMn69lnn1VkZKRq166t2bNnq0SJEvrkk0+y7f/FF19o4MCBatiwoUJCQvTRRx8pIyNDsbGxDo4cAAAAAAAAeeXUolRaWpq2b9+uiIgIc5ubm5siIiK0efNmq45x+fJlXbt2TWXKlMmvMAEAAAAAAGBnTl1T6uzZs0pPT5e/v79Fu7+/v/bt22fVMV577TUFBARYFLZulpqaqtTUVPPr5OTkvAcMAABQgJDnAACAwszpt+/diYkTJ+rLL7/UsmXLVKxYsWz7xMTEyNfX17wFBgY6OEoAAID8QZ4DAAAKM6cWpcqVKyd3d3clJCRYtCckJKhChQq33fff//63Jk6cqDVr1qh+/fo59hs1apSSkpLM24kTJ+wSOwAAgLOR5wAAgMLMqUUpDw8PhYaGWixSnrloeXh4eI77vfvuu3rrrbe0atUqNWnS5Lbn8PT0lI+Pj8UGAADgCshzAABAYebUNaUkKSoqSn379lWTJk3UtGlTTZ06VSkpKYqMjJQk9enTR5UqVVJMTIwkadKkSRozZowWLlyooKAgxcfHS5JKliypkiVLOm0cAAAAAAAAsJ7Ti1I9evTQmTNnNGbMGMXHx6thw4ZatWqVefHz48ePy83tfxO6Zs2apbS0NHXt2tXiONHR0Ro7dqwjQwcAAAAAAEAeOb0oJUmDBw/W4MGDs31v48aNFq+PHj2a/wEBAAAAAAAgXxXqp+8BAAAAAACgcCoQM6UA2C5o5Epnh5CroxM7OTsEAAAAAEABxUwpAAAAAAAAOBxFKQAAAAAAADgcRSkAAAAAAAA4HEUpAAAAAAAAOBxFKQAAAAAAADgcRSkAAAAAAAA4XBFnBwAAAO5OQSNXOjuEXB2d2MnZISCfFfS/Q/4GAQCujJlSAAAAAAAAcDiKUgAAAAAAAHA4ilIAAAAAAABwOIpSAAAAAAAAcDiKUgAAAAAAAHA4ilIAAAAAAABwOIpSAAAAAAAAcDiKUgAAAAAAAHA4ilIAAAAAAABwOIpSAAAAAAAAcDiKUgAAAAAAAHA4ilIAAAAAAABwOIpSAAAAAAAAcDiKUgAAAAAAAHA4ilIAAAAAAABwOIpSAAAAAAAAcDiKUgAAAAAAAHC4Is4OAAAAWC9o5Epnh5CroxM7OTsEAAAAFALMlAIAAAAAAIDDMVMKAAAANinoM/aYrQcAQOHATCkAAAAAAAA4HEUpAAAAAAAAOBxFKQAAAAAAADgcRSkAAAAAAAA4HEUpAAAAAAAAOBxFKQAAAAAAADgcRSkAAAAAAAA4XBFnBwAAAAAAmYJGrnR2CLk6OrGTs0NwOD4XAPmBmVIAAAAAAABwOIpSAAAAAAAAcDhu3wPgdK40HdyVxgIAAAAA+YmZUgAAAAAAAHA4ZkoBAAAAhRwzdQEAhREzpQAAAAAAAOBwFKUAAAAAAADgcBSlAAAAAAAA4HAUpQAAAAAAAOBwLHQOAAAAAABgJzx8wnrMlAIAAAAAAIDDUZQCAAAAAACAw1GUAgAAAAAAgMMViKLUzJkzFRQUpGLFiiksLEy///77bft/9dVXCgkJUbFixVSvXj19//33DooUAAAAAAAA9uD0hc4XLVqkqKgozZ49W2FhYZo6dao6dOigv//+W35+fln6//rrr+rVq5diYmL08MMPa+HChXrssce0Y8cO1a1b1wkjAADXxAKNAADcGb5LAeD2nF6Umjx5sp599llFRkZKkmbPnq2VK1fqk08+0ciRI7P0nzZtmh588EGNGDFCkvTWW29p7dq1mjFjhmbPnu3Q2AEAAAAAhQvFQqDgcOrte2lpadq+fbsiIiLMbW5uboqIiNDmzZuz3Wfz5s0W/SWpQ4cOOfYHAAAAAABAwePUmVJnz55Venq6/P39Ldr9/f21b9++bPeJj4/Ptn98fHy2/VNTU5Wammp+nZSUJElKTk6+k9BzlZF6OV+Pbw/WXgNXGkvd6NX5HMmd2z2ug1X9XOlzYSyOdTeOxZW40ufiSmO50+MbhpGn/clzsmfL+F1lLAV9HBJjKagYS8HEWAqmu3Esd3r8XHMcw4lOnjxpSDJ+/fVXi/YRI0YYTZs2zXafokWLGgsXLrRomzlzpuHn55dt/+joaEMSGxsbGxsbG1uB3U6cOJGnXIo8h42NjY2Nja0gb7nlOE6dKVWuXDm5u7srISHBoj0hIUEVKlTIdp8KFSrY1H/UqFGKiooyv87IyFBiYqLKli0rk8l0hyNwnOTkZAUGBurEiRPy8fFxdjh3hLEUTIyl4HGVcUiMpaBiLM5nGIYuXryogICAPO3vCnlOYf3sssNYCibGUjAxloKJsRRMhXEs1uY4Ti1KeXh4KDQ0VLGxsXrsscck3UimYmNjNXjw4Gz3CQ8PV2xsrIYOHWpuW7t2rcLDw7Pt7+npKU9PT4u2UqVK2SN8p/Dx8Sk0f4S5YSwFE2MpeFxlHBJjKagYi3P5+vrmeV9XynMK42eXE8ZSMDGWgomxFEyMpWAqbGOxJsdx+tP3oqKi1LdvXzVp0kRNmzbV1KlTlZKSYn4aX58+fVSpUiXFxMRIkoYMGaJWrVrp/fffV6dOnfTll19q27ZtmjNnjjOHAQAAAAAAABs4vSjVo0cPnTlzRmPGjFF8fLwaNmyoVatWmRczP378uNzc/veQwGbNmmnhwoV688039frrr6tmzZpavny56tat66whAAAAAAAAwEZOL0pJ0uDBg3O8XW/jxo1Z2rp166Zu3brlc1QFi6enp6Kjo7NM0S+MGEvBxFgKHlcZh8RYCirGgoLAlT47xlIwMZaCibEUTIylYHKlsdzKZBh5fAYxAAAAAAAAkEduuXcBAAAAAAAA7IuiFAAAAAAAAByOohQAAAAAAAAcjqJUITBz5kwFBQWpWLFiCgsL0++//+7skPJk06ZN6ty5swICAmQymbR8+XJnh5QnMTExuu++++Tt7S0/Pz899thj+vvvv50dVp7MmjVL9evXl4+Pj3x8fBQeHq4ffvjB2WHZxcSJE2UymTR06FBnh2KzsWPHymQyWWwhISHODivPTp48qaeeekply5ZV8eLFVa9ePW3bts3ZYdksKCgoy+diMpk0aNAgZ4dms/T0dI0ePVrVqlVT8eLFVaNGDb311lsqrMtMXrx4UUOHDlXVqlVVvHhxNWvWTFu3bnV2WLACOU7B4ko5juS6eQ45TsFBjlPwkOMUPhSlCrhFixYpKipK0dHR2rFjhxo0aKAOHTro9OnTzg7NZikpKWrQoIFmzpzp7FDuyI8//qhBgwbpt99+09q1a3Xt2jW1b99eKSkpzg7NZpUrV9bEiRO1fft2bdu2Tf/617/06KOP6q+//nJ2aHdk69at+vDDD1W/fn1nh5JnderU0alTp8zbzz//7OyQ8uT8+fNq3ry5ihYtqh9++EF79uzR+++/r9KlSzs7NJtt3brV4jNZu3atJBXKp8FOmjRJs2bN0owZM7R3715NmjRJ7777rqZPn+7s0PLkmWee0dq1a7VgwQLt2rVL7du3V0REhE6ePOns0HAb5DgFjyvlOJJr5jnkOAUHOU7BRI5TCBko0Jo2bWoMGjTI/Do9Pd0ICAgwYmJinBjVnZNkLFu2zNlh2MXp06cNScaPP/7o7FDsonTp0sZHH33k7DDy7OLFi0bNmjWNtWvXGq1atTKGDBni7JBsFh0dbTRo0MDZYdjFa6+9ZrRo0cLZYeSLIUOGGDVq1DAyMjKcHYrNOnXqZPTv39+irUuXLsaTTz7ppIjy7vLly4a7u7vx3XffWbQ3btzYeOONN5wUFaxBjlPwuVqOYxiFO88hxylYyHEKJnKcwoeZUgVYWlqatm/froiICHObm5ubIiIitHnzZidGhpslJSVJksqUKePkSO5Menq6vvzyS6WkpCg8PNzZ4eTZoEGD1KlTJ4v/bgqjAwcOKCAgQNWrV9eTTz6p48ePOzukPPn222/VpEkTdevWTX5+fmrUqJHmzp3r7LDuWFpamj7//HP1799fJpPJ2eHYrFmzZoqNjdX+/fslSTt37tTPP/+sjh07Ojky212/fl3p6ekqVqyYRXvx4sUL7a/vdwNynMLBVXIcyTXyHHKcgoUcp2Aixyl8ijg7AOTs7NmzSk9Pl7+/v0W7v7+/9u3b56SocLOMjAwNHTpUzZs3V926dZ0dTp7s2rVL4eHhunr1qkqWLKlly5apdu3azg4rT7788kvt2LGj0N9nHRYWpk8//VT33nuvTp06pXHjxqlly5bavXu3vL29nR2eTQ4fPqxZs2YpKipKr7/+urZu3aqXX35ZHh4e6tu3r7PDy7Ply5frwoUL6tevn7NDyZORI0cqOTlZISEhcnd3V3p6ut555x09+eSTzg7NZt7e3goPD9dbb72lWrVqyd/fX//3f/+nzZs3Kzg42NnhIQfkOAWfK+Q4kuvkOeQ4BQ85TsFEjlP4UJQC7sCgQYO0e/fuQl2pvvfeexUXF6ekpCQtWbJEffv21Y8//ljoErYTJ05oyJAhWrt2bZZfEwqbm3/JqV+/vsLCwlS1alUtXrxYAwYMcGJktsvIyFCTJk00YcIESVKjRo20e/duzZ49u1AnbB9//LE6duyogIAAZ4eSJ4sXL9YXX3yhhQsXqk6dOoqLi9PQoUMVEBBQKD+XBQsWqH///qpUqZLc3d3VuHFj9erVS9u3b3d2aECh5Qo5juQaeQ45TsFEjlMwkeMUPhSlCrBy5crJ3d1dCQkJFu0JCQmqUKGCk6JCpsGDB+u7777Tpk2bVLlyZWeHk2ceHh7mSntoaKi2bt2qadOm6cMPP3RyZLbZvn27Tp8+rcaNG5vb0tPTtWnTJs2YMUOpqalyd3d3YoR5V6pUKd1zzz06ePCgs0OxWcWKFbMk/rVq1dLXX3/tpIju3LFjx7Ru3TotXbrU2aHk2YgRIzRy5Ej17NlTklSvXj0dO3ZMMTExhTJhq1Gjhn788UelpKQoOTlZFStWVI8ePVS9enVnh4YckOMUbK6S40iukeeQ4xRM5DgFEzlO4cOaUgWYh4eHQkNDFRsba27LyMhQbGxsob0X3hUYhqHBgwdr2bJlWr9+vapVq+bskOwqIyNDqampzg7DZm3bttWuXbsUFxdn3po0aaInn3xScXFxhTZZk6RLly7p0KFDqlixorNDsVnz5s2zPE58//79qlq1qpMiunPz5s2Tn5+fOnXq5OxQ8uzy5ctyc7NMAdzd3ZWRkeGkiOzDy8tLFStW1Pnz57V69Wo9+uijzg4JOSDHKZhcPceRCmeeQ45TMJHjFEzkOIUPM6UKuKioKPXt21dNmjRR06ZNNXXqVKWkpCgyMtLZodns0qVLFr+CHDlyRHFxcSpTpoyqVKnixMhsM2jQIC1cuFDffPONvL29FR8fL0ny9fVV8eLFnRydbUaNGqWOHTuqSpUqunjxohYuXKiNGzdq9erVzg7NZt7e3lnWvPDy8lLZsmUL3VoYw4cPV+fOnVW1alX9888/io6Olru7u3r16uXs0Gw2bNgwNWvWTBMmTFD37t31+++/a86cOZozZ46zQ8uTjIwMzZs3T3379lWRIoX3K7Rz58565513VKVKFdWpU0d//PGHJk+erP79+zs7tDxZvXq1DMPQvffeq4MHD2rEiBEKCQkplN+VdxNynILHlXIcyXXyHHKcgokcp2AixymEnPvwP1hj+vTpRpUqVQwPDw+jadOmxm+//ebskPJkw4YNhqQsW9++fZ0dmk2yG4MkY968ec4OzWb9+/c3qlatanh4eBjly5c32rZta6xZs8bZYdlNYX1cco8ePYyKFSsaHh4eRqVKlYwePXoYBw8edHZYebZixQqjbt26hqenpxESEmLMmTPH2SHl2erVqw1Jxt9//+3sUO5IcnKyMWTIEKNKlSpGsWLFjOrVqxtvvPGGkZqa6uzQ8mTRokVG9erVDQ8PD6NChQrGoEGDjAsXLjg7LFiBHKdgcaUcxzBcO88hxykYyHEKHnKcwsdkGIbhuBIYAAAAAAAAwJpSAAAAAAAAcAKKUgAAAAAAAHA4ilIAAAAAAABwOIpSAAAAAAAAcDiKUgAAAAAAAHA4ilIAAAAAAABwOIpSAAAAAAAAcDiKUgAAAAAAAHA4ilIA7iomk0nLly+XJB09elQmk0lxcXFOjQkAAMAeyHMAFDYUpQC4jPj4eL300kuqXr26PD09FRgYqM6dOys2Njbb/oGBgTp16pTq1q1r1zhuTght6Wcymcybl5eXatasqX79+mn79u12jQ8AABQ+5DkAXBFFKQAu4ejRowoNDdX69ev13nvvadeuXVq1apXatGmjQYMGZbuPu7u7KlSooCJFijg42pzNmzdPp06d0l9//aWZM2fq0qVLCgsL0/z5850dGgAAcBLyHACuiqIUAJcwcOBAmUwm/f7773riiSd0zz33qE6dOoqKitJvv/2W7T7ZTWvfvXu3OnbsqJIlS8rf319PP/20zp49a36/devWevnll/Xqq6+qTJkyqlChgsaOHWt+PygoSJL0+OOPy2QymV9bq1SpUqpQoYKCgoLUvn17LVmyRE8++aQGDx6s8+fP23QsAADgGshzALgqilIACr3ExEStWrVKgwYNkpeXV5b3S5UqZdVxLly4oH/9619q1KiRtm3bplWrVikhIUHdu3e36PfZZ5/Jy8tLW7Zs0bvvvqvx48dr7dq1kqStW7dK+t8vgZmv78SwYcN08eJF8zkAAMDdgzwHgCsrOHM5ASCPDh48KMMwFBISckfHmTFjhho1aqQJEyaY2z755BMFBgZq//79uueeeyRJ9evXV3R0tCSpZs2amjFjhmJjY9WuXTuVL19e0v9+CbSHzHEdPXrULscDAACFB3kOAFdGUQpAoWcYhl2Os3PnTm3YsEElS5bM8t6hQ4cskrWbVaxYUadPn7ZLDNnJHJ/JZMq3cwAAgIKJPAeAK6MoBaDQq1mzpkwmk/bt23dHx7l06ZI6d+6sSZMmZXmvYsWK5n8XLVrU4j2TyaSMjIw7Ovft7N27V5JUrVq1fDsHAAAomMhzALgy1pQCUOiVKVNGHTp00MyZM5WSkpLl/QsXLlh1nMaNG+uvv/5SUFCQgoODLbbs1nDISdGiRZWenm51/9xMnTpVPj4+ioiIsNsxAQBA4UCeA8CVUZQC4BJmzpyp9PR0NW3aVF9//bUOHDigvXv36j//+Y/Cw8OtOsagQYOUmJioXr16aevWrTp06JBWr16tyMhIm5KvoKAgxcbGKj4+3uYnyVy4cEHx8fE6duyY1q5dq65du2rhwoWaNWuW1QuZAgAA10KeA8BVUZQC4BKqV6+uHTt2qE2bNnrllVdUt25dtWvXTrGxsZo1a5ZVxwgICNAvv/yi9PR0tW/fXvXq1dPQoUNVqlQpublZ/z+X77//vtauXavAwEA1atTIpnFERkaqYsWKCgkJ0YsvvqiSJUvq999/V+/evW06DgAAcB3kOQBclcmw18p5AAAAAAAAgJWYKQUAAAAAAACHoygFAAAAAAAAh6MoBQAAAAAAAIejKAUAAAAAAACHoygFAAAAAAAAh6MoBQAAAAAAAIejKAUAAAAAAACHoygFAAAAAAAAh6MoBQAAAAAAAIejKAUAAAAAAACHoygFAAAAAAAAh6MoBQAAAAAAAIf7f9o0ylCOibzCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "available_classes = sorted(client_data.keys())\n",
    "n_plot = min(2, len(available_classes))\n",
    "classes_to_plot = random.sample(available_classes, n_plot)\n",
    "\n",
    "counts_list = []\n",
    "for c in classes_to_plot:\n",
    "    chunks = client_data[c]\n",
    "    counts = np.array([len(chunk) for chunk in chunks], dtype=int)\n",
    "    counts_list.append(counts)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_plot, figsize=(6 * n_plot, 4), sharey=True)\n",
    "if n_plot == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, c, counts in zip(axes, classes_to_plot, counts_list):\n",
    "    frac = counts / counts.sum()          # normalize to [0,1]\n",
    "    ax.bar(np.arange(len(frac)), frac)\n",
    "    ax.set_title(f\"Class {c} (fraction per client)\")\n",
    "    ax.set_xlabel(\"Client ID\")\n",
    "    ax.set_xticks(np.arange(len(frac)))\n",
    "    ax.set_ylim(0, 1.0)                   # same y-axis for all\n",
    "\n",
    "axes[0].set_ylabel(\"Fraction of this class\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T12:27:22.638847Z",
     "iopub.status.busy": "2025-11-16T12:27:22.638544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FemnistCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout2d): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=62, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 428,350\n",
      "Trainable parameters: 428,350\n",
      "\n",
      "Test input shape: torch.Size([4, 1, 28, 28])\n",
      "Test output shape: torch.Size([4, 62])\n",
      "Total samples assigned to clients: 651421\n",
      "Train set size: 651421\n",
      "Collecting torch_optimizer\n",
      "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torch_optimizer) (2.6.0+cu124)\n",
      "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\n",
      "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (2025.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.5.0->torch_optimizer)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.5.0->torch_optimizer)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.5.0->torch_optimizer)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.5.0->torch_optimizer)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.5.0->torch_optimizer)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.5.0->torch_optimizer)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.5.0->torch_optimizer)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.5.0->torch_optimizer)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.5.0->torch_optimizer)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.5.0->torch_optimizer)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (3.0.3)\n",
      "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-ranger, torch_optimizer\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n",
      "Created 10 clients and 1 server.\n",
      "\n",
      "=== Round 1 ===\n",
      "Selected clients: [6, 3, 0, 5, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 6: trained on 46288 samples, loss=1.5680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 3: trained on 76167 samples, loss=1.3771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: trained on 65873 samples, loss=1.3326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 5: trained on 75084 samples, loss=1.1975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 8: trained on 68706 samples, loss=1.3980\n",
      "Round 1 aggregated training loss = 1.3586\n",
      "Server test -> loss: 1.1964, acc: 0.7208\n",
      "\n",
      "=== Round 2 ===\n",
      "Selected clients: [3, 6, 8, 0, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 3: trained on 76167 samples, loss=0.6539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 6: trained on 46288 samples, loss=0.6354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 8: trained on 68706 samples, loss=0.6750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 0 | Epoch 1/10:  57%|    | 1169/2059 [00:18<00:14, 63.22it/s, loss=1.0872]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "Client 0 | Epoch 9/10:  74%|  | 1526/2059 [00:23<00:08, 63.29it/s, loss=0.6205]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "Client 2 | Epoch 8/10:  38%|      | 840/2233 [00:13<00:21, 65.38it/s, loss=0.3558] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 6: trained on 46288 samples, loss=0.5072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 2 | Epoch 4/10:  11%|        | 252/2233 [00:04<00:31, 62.86it/s, loss=0.4547] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2: trained on 71451 samples, loss=0.5715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 4 | Epoch 2/10:  15%|        | 279/1866 [00:04<00:24, 63.91it/s, loss=0.5498] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 4: trained on 59704 samples, loss=0.6343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 3 | Epoch 1/10:   0%|          | 7/2381 [00:00<00:37, 63.04it/s, loss=0.3420]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "Client 3 | Epoch 8/10:  13%|        | 308/2381 [00:04<00:32, 64.42it/s, loss=0.5522] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 8: trained on 68706 samples, loss=0.5632\n",
      "Round 3 aggregated training loss = 0.5679\n",
      "Server test -> loss: 0.5081, acc: 0.8275\n",
      "\n",
      "=== Round 4 ===\n",
      "Selected clients: [2, 1, 0, 6, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 2 | Epoch 2/10:  28%|       | 623/2233 [00:09<00:24, 64.44it/s, loss=1.1539] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "Client 2 | Epoch 9/10:  57%|    | 1281/2233 [00:19<00:14, 65.53it/s, loss=0.5509]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "Client 0 | Epoch 7/10:  87%| | 1788/2059 [00:29<00:04, 60.85it/s, loss=0.3833]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "Client 1 | Epoch 6/10:  18%|        | 420/2278 [00:06<00:28, 65.39it/s, loss=0.2751] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1: trained on 72888 samples, loss=0.4911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 5 | Epoch 3/10:   2%|         | 56/2347 [00:00<00:36, 62.92it/s, loss=0.4549]  IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "Client 7 | Epoch 9/10:  73%|  | 1379/1879 [00:21<00:07, 65.71it/s, loss=0.4185]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "Client 2 | Epoch 7/10:  33%|      | 728/2233 [00:11<00:23, 64.38it/s, loss=0.5392] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 6: trained on 46288 samples, loss=0.4099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 3 | Epoch 3/10:  84%| | 2009/2381 [00:32<00:05, 62.43it/s, loss=0.5466]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: trained on 65873 samples, loss=0.3793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 5 | Epoch 7/10:  38%|      | 895/2347 [00:14<00:22, 63.86it/s, loss=0.3645] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "Client 9 | Epoch 9/10:  81%|  | 1392/1724 [00:22<00:05, 61.14it/s, loss=0.2857]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 7: trained on 60109 samples, loss=0.3394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 6 | Epoch 4/10:  39%|      | 567/1447 [00:09<00:13, 63.66it/s, loss=0.2570] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 6: trained on 46288 samples, loss=0.3716\n",
      "Round 9 aggregated training loss = 0.3958\n",
      "Server test -> loss: 0.4282, acc: 0.8453\n",
      "\n",
      "=== Round 10 ===\n",
      "Selected clients: [5, 1, 0, 9, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 5 | Epoch 1/10:  21%|        | 496/2347 [00:07<00:29, 61.92it/s, loss=0.3121]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "Client 4 | Epoch 6/10:   6%|         | 112/1866 [00:01<00:27, 64.83it/s, loss=0.6702] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 4: trained on 59704 samples, loss=0.4952\n",
      "Round 10 aggregated training loss = 0.4156\n",
      "Server test -> loss: 0.4080, acc: 0.8584\n",
      "\n",
      "=== Round 11 ===\n",
      "Selected clients: [5, 9, 6, 7, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 5 | Epoch 1/10:  47%|     | 1112/2347 [00:17<00:19, 63.79it/s, loss=0.6853]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 7: trained on 60109 samples, loss=0.3252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 1 | Epoch 4/10:  25%|       | 560/2278 [00:08<00:26, 65.47it/s, loss=0.5217] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "Client 1 | Epoch 8/10:  52%|    | 1175/2278 [00:18<00:17, 62.53it/s, loss=0.4432]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1: trained on 72888 samples, loss=0.4199\n",
      "Round 11 aggregated training loss = 0.3819\n",
      "Server test -> loss: 0.4123, acc: 0.8547\n",
      "\n",
      "=== Round 12 ===\n",
      "Selected clients: [4, 7, 8, 6, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 4 | Epoch 9/10:  27%|       | 504/1866 [00:07<00:22, 61.90it/s, loss=0.3543] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 5: trained on 75084 samples, loss=0.3302\n",
      "Round 13 aggregated training loss = 0.3714\n",
      "Server test -> loss: 0.4078, acc: 0.8539\n",
      "\n",
      "=== Round 14 ===\n",
      "Selected clients: [7, 3, 2, 4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 7 | Epoch 5/10:   2%|         | 35/1879 [00:00<00:28, 65.05it/s, loss=0.3473]  IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=1.0 (secs)\n",
      "\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 6: trained on 46288 samples, loss=0.3408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 3 | Epoch 2/10:  81%|  | 1918/2381 [00:30<00:07, 63.47it/s, loss=0.1905]"
     ]
    }
   ],
   "source": [
    "class FemnistCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        # Input: (N, 1, 28, 28)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)  # halves H and W\n",
    "        self.dropout2d = nn.Dropout2d(p=0.25)\n",
    "\n",
    "        # After two pools: 28 -> 14 -> 7, channels = 64\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)       # (N, 32, 14, 14)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)       # (N, 64, 7, 7)\n",
    "        x = self.dropout2d(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)  # (N, 64*7*7)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)          # (N, num_classes)\n",
    "        return x\n",
    "num_classes = len(train_data.features[\"character\"].names)\n",
    "model = FemnistCNN(num_classes=num_classes).to(device)\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "test_input = torch.randn(4, 1, 28, 28).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"\\nTest input shape: {test_input.shape}\")\n",
    "print(f\"Test output shape: {test_output.shape}\")\n",
    "# %% [code]\n",
    "# Build per-client index lists from the per-class Dirichlet splits (client_data)\n",
    "\n",
    "num_clients = 10  # already used above, but restate for clarity\n",
    "\n",
    "client_indices = [[] for _ in range(num_clients)]\n",
    "\n",
    "for c, chunks in client_data.items():\n",
    "    for cid in range(num_clients):\n",
    "        client_indices[cid].extend(chunks[cid].tolist())\n",
    "\n",
    "# Shuffle indices within each client for randomness\n",
    "for cid in range(num_clients):\n",
    "    random.shuffle(client_indices[cid])\n",
    "\n",
    "# Sanity check\n",
    "total_assigned = sum(len(idxs) for idxs in client_indices)\n",
    "print(f\"Total samples assigned to clients: {total_assigned}\")\n",
    "print(f\"Train set size: {len(train_data)}\")\n",
    "\n",
    "\n",
    "!pip install torch_optimizer\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch_optimizer as topt\n",
    "from tqdm import tqdm\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),   \n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "class FemnistTorchDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, indices, transform=None):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.indices = list(indices)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.hf_dataset[self.indices[idx]]\n",
    "        img = ex[\"image\"]       # PIL image\n",
    "        label = int(ex[\"character\"])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)  # -> (1, 28, 28) tensor\n",
    "        return img, label\n",
    "\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, model: nn.Module, lr: float = 0.01):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.model_size=sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "\n",
    "class Client(Node):\n",
    "    def __init__(\n",
    "        self,\n",
    "        client_id,\n",
    "        model,\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        optimizer_name,\n",
    "        optimizer_args,\n",
    "        device: torch.device = device,\n",
    "    ):\n",
    "        super().__init__(model, lr=optimizer_args.get(\"lr\", 0.01) if optimizer_args else 0.01)\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.optimizer_name = optimizer_name.lower()\n",
    "        self.optimizer_args = optimizer_args or {}\n",
    "\n",
    "        self.optimizer = self._make_optimizer(self.optimizer_name, self.optimizer_args)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _make_optimizer(self, name: str, kwargs: dict):\n",
    "        params = self.model.parameters()\n",
    "        name = name.lower()\n",
    "\n",
    "        if name == \"sgd\":\n",
    "            return torch.optim.SGD(params, lr=self.lr)\n",
    "        elif name == \"sgd_momentum\":\n",
    "            momentum = kwargs.get(\"momentum\", 0.9)\n",
    "            nesterov = kwargs.get(\"nesterov\", False)\n",
    "            return torch.optim.SGD(params, lr=self.lr, momentum=momentum, nesterov=nesterov)\n",
    "        elif name == \"adam\":\n",
    "            return torch.optim.Adam(params, lr=kwargs.get(\"lr\", 0.001))\n",
    "        elif name == \"adamw\":\n",
    "            return torch.optim.AdamW(params, lr=kwargs.get(\"lr\", 0.001))\n",
    "        elif name == \"rmsprop\":\n",
    "            return torch.optim.RMSprop(params, lr=kwargs.get(\"lr\", 0.001))\n",
    "        elif name == \"radam\":\n",
    "            return topt.RAdam(params, lr=kwargs.get(\"lr\", 0.001))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {name}\")\n",
    "\n",
    "    def receive_from_server(self, global_state_dict: dict):\n",
    "        self.model.load_state_dict(global_state_dict)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    \n",
    "\n",
    "    def train_local(self, local_epochs: int = 1):\n",
    "        self.model.train()\n",
    "        total_samples = 0\n",
    "        total_loss = 0.0\n",
    "    \n",
    "        # Outer loop = epochs\n",
    "        for epoch in range(local_epochs):\n",
    "            pbar = tqdm(self.dataloader, desc=f\"Client {self.client_id} | Epoch {epoch+1}/{local_epochs}\", leave=False)\n",
    "            \n",
    "            # Inner loop = batches\n",
    "            for xb, yb in pbar:\n",
    "                xb = xb.to(self.device)\n",
    "                yb = yb.to(self.device)\n",
    "    \n",
    "                logits = self.model(xb)\n",
    "                loss = self.criterion(logits, yb)\n",
    "    \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "                batch_size = xb.size(0)\n",
    "                total_samples += batch_size\n",
    "                total_loss += loss.item() * batch_size\n",
    "    \n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    \"loss\": f\"{loss.item():.4f}\",\n",
    "                })\n",
    "    \n",
    "        avg_loss = total_loss / total_samples\n",
    "        return total_samples, avg_loss\n",
    "\n",
    "\n",
    "    def send_to_server(self):\n",
    "    \n",
    "        state = {k: v.detach().cpu().clone() for k, v in self.model.state_dict().items()}\n",
    "        n = len(self.dataset)\n",
    "        return state, n\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        loss_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(self.device)\n",
    "                yb = yb.to(self.device)\n",
    "\n",
    "                logits = self.model(xb)\n",
    "                loss = self.criterion(logits, yb)\n",
    "\n",
    "                loss_sum += loss.item() * xb.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == yb).sum().item()\n",
    "                total += xb.size(0)\n",
    "\n",
    "        return loss_sum / total, correct / total\n",
    "\n",
    "\n",
    "class Server(Node):\n",
    "    def __init__(self, model, test_dataset, device):\n",
    "        super().__init__(model, lr=0.01)\n",
    "        self.device = device\n",
    "        self.test_dataset = test_dataset\n",
    "        self.model.to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def update_model(self, client_states):\n",
    "        if len(client_states) == 0:\n",
    "            return\n",
    "\n",
    "        total_samples = sum(n for _, n in client_states)\n",
    "        new_state = {}\n",
    "        first_state = client_states[0][0]\n",
    "        for k, v in first_state.items():\n",
    "            new_state[k] = torch.zeros_like(v)\n",
    "\n",
    "        for state, n in client_states:\n",
    "            weight = n / total_samples\n",
    "            for k, v in state.items():\n",
    "                new_state[k] += weight * v\n",
    "\n",
    "        new_state = {k: v.to(self.device) for k, v in new_state.items()}\n",
    "        self.model.load_state_dict(new_state)\n",
    "\n",
    "    def test(self):\n",
    "        \n",
    "        self.model.eval()\n",
    "        loader = DataLoader(self.test_dataset, batch_size=128, shuffle=False)\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        loss_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(self.device)\n",
    "                yb = yb.to(self.device)\n",
    "\n",
    "                logits = self.model(xb)\n",
    "                loss = self.criterion(logits, yb)\n",
    "\n",
    "                loss_sum += loss.item() * xb.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == yb).sum().item()\n",
    "                total += xb.size(0)\n",
    "\n",
    "        return loss_sum / total, correct / total\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "num_clients = 10 \n",
    "\n",
    "client_datasets = []\n",
    "for cid in range(num_clients):\n",
    "    ds = FemnistTorchDataset(train_data, client_indices[cid], transform=transform)\n",
    "    client_datasets.append(ds)\n",
    "\n",
    "test_dataset = FemnistTorchDataset(test_data, list(range(len(test_data))), transform=transform)\n",
    "\n",
    "\n",
    "clients = []\n",
    "for cid in range(num_clients):\n",
    "    m = FemnistCNN(num_classes=num_classes).to(device)\n",
    "    client = Client(\n",
    "        client_id=cid,\n",
    "        model=m,\n",
    "        dataset=client_datasets[cid],\n",
    "        batch_size=batch_size,\n",
    "        optimizer_name=\"sgd\",       # change to 'sgd_momentum', 'adam', 'adamw', 'rmsprop', 'radam'\n",
    "        optimizer_args={},        # change lr, momentum etc \n",
    "        device=device,\n",
    "    )\n",
    "    clients.append(client)\n",
    "\n",
    "server_model = FemnistCNN(num_classes=num_classes).to(device)\n",
    "server = Server(server_model, test_dataset, device=device)\n",
    "\n",
    "print(f\"Created {len(clients)} clients and 1 server.\")\n",
    "\n",
    "# %% [code]\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "\n",
    "T = 20          # global rounds\n",
    "K = 10          # local epochs per selected client\n",
    "m = 5           # number of clients selected per round\n",
    "results_csv = \"fedavg_results_with_trainloss.csv\"\n",
    "ckpt_dir = \"checkpoints\"\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "with open(results_csv, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    header = [\"round\", \"train_loss\", \"global_loss\", \"global_acc\"]\n",
    "    writer.writerow(header)\n",
    "\n",
    "for rnd in range(1, T + 1):\n",
    "    print(f\"\\n=== Round {rnd} ===\")\n",
    "    \n",
    "    # 1) sample clients\n",
    "    selected_ids = random.sample(range(num_clients), m)\n",
    "    print(\"Selected clients:\", selected_ids)\n",
    "\n",
    "    # 2) broadcast global model\n",
    "    global_state = {k: v.cpu().clone() for k, v in server.model.state_dict().items()}\n",
    "    for cid in selected_ids:\n",
    "        clients[cid].receive_from_server(global_state)\n",
    "\n",
    "    # 3) local training and upload\n",
    "    client_states = []\n",
    "    round_losses = []   # (loss, n_samples) for this round\n",
    "    for cid in selected_ids:\n",
    "        n_local, client_loss = clients[cid].train_local(local_epochs=K)\n",
    "        state, n_samples = clients[cid].send_to_server()\n",
    "        client_states.append((state, n_samples))\n",
    "        round_losses.append((client_loss, n_local))\n",
    "        print(f\"Client {cid}: trained on {n_samples} samples, loss={client_loss:.4f}\")\n",
    "\n",
    "    # 4) aggregated training loss\n",
    "    total_trained = sum(n for (_, n) in round_losses)\n",
    "    train_loss_round = sum(loss * n for (loss, n) in round_losses) / total_trained\n",
    "    print(f\"Round {rnd} aggregated training loss = {train_loss_round:.4f}\")\n",
    "\n",
    "    # 5) server aggregation\n",
    "    server.update_model(client_states)\n",
    "\n",
    "    # 6) global test\n",
    "    g_loss, g_acc = server.test()\n",
    "    print(f\"Server test -> loss: {g_loss:.4f}, acc: {g_acc:.4f}\")\n",
    "\n",
    "    # 7) save checkpoint\n",
    "    ckpt_path = os.path.join(ckpt_dir, f\"server_round_{rnd}.pt\")\n",
    "    server.save_checkpoint(ckpt_path)\n",
    "    # 8) log to CSV\n",
    "    row = [rnd, train_loss_round, g_loss, g_acc]\n",
    "    with open(results_csv, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Training finished. Results saved to:\", results_csv)\n",
    "\n",
    "# %% [code]\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_csv = \"fedavg_results_with_trainloss.csv\"\n",
    "df = pd.read_csv(results_csv)\n",
    "\n",
    "rounds = df[\"round\"]\n",
    "\n",
    "# --- Training Loss vs Rounds ---\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rounds, df[\"train_loss\"], linewidth=2)\n",
    "plt.title(\"Training Loss vs Rounds\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Test Loss vs Rounds ---\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rounds, df[\"global_loss\"], linewidth=2)\n",
    "plt.title(\"Test Loss vs Rounds\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Test Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Test Accuracy vs Rounds ---\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rounds, df[\"global_acc\"], linewidth=2)\n",
    "plt.title(\"Test Accuracy vs Rounds\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
